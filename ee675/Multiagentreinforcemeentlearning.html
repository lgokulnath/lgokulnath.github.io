<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_51zhi3wgordc-0>li{counter-increment:lst-ctn-kix_51zhi3wgordc-0}.lst-kix_u8ma5k7yjif9-4>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-4}ol.lst-kix_dloyb1hvabfn-4.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-4 0}ul.lst-kix_yq7ycl3ov8vy-8{list-style-type:none}ul.lst-kix_yq7ycl3ov8vy-7{list-style-type:none}ul.lst-kix_yq7ycl3ov8vy-6{list-style-type:none}ul.lst-kix_yq7ycl3ov8vy-5{list-style-type:none}.lst-kix_99yhmmrvcmks-0>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-0,lower-latin) ". "}ul.lst-kix_yq7ycl3ov8vy-0{list-style-type:none}ol.lst-kix_99yhmmrvcmks-2.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-2 0}.lst-kix_99yhmmrvcmks-2>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-2,decimal) ". "}.lst-kix_a009cfjyoj27-1>li{counter-increment:lst-ctn-kix_a009cfjyoj27-1}ul.lst-kix_yq7ycl3ov8vy-4{list-style-type:none}.lst-kix_99yhmmrvcmks-1>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-1,lower-roman) ". "}ul.lst-kix_yq7ycl3ov8vy-3{list-style-type:none}ul.lst-kix_yq7ycl3ov8vy-2{list-style-type:none}ul.lst-kix_yq7ycl3ov8vy-1{list-style-type:none}ol.lst-kix_xc0vy64nvluu-6.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-6 0}.lst-kix_dmij6bobu93-4>li{counter-increment:lst-ctn-kix_dmij6bobu93-4}ol.lst-kix_u8ma5k7yjif9-6.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-6 0}.lst-kix_vh7w5tnlpfsm-8>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-8}ol.lst-kix_51zhi3wgordc-0.start{counter-reset:lst-ctn-kix_51zhi3wgordc-0 0}ol.lst-kix_vh7w5tnlpfsm-4.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-4 0}ol.lst-kix_51zhi3wgordc-7.start{counter-reset:lst-ctn-kix_51zhi3wgordc-7 0}.lst-kix_jttb5ey0mtdr-0>li:before{content:"-  "}.lst-kix_99yhmmrvcmks-7>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-7,lower-roman) ". "}.lst-kix_jttb5ey0mtdr-3>li:before{content:"-  "}.lst-kix_jttb5ey0mtdr-4>li:before{content:"-  "}.lst-kix_kh29jsq0h4s-1>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-1}.lst-kix_99yhmmrvcmks-6>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-6,lower-latin) ". "}.lst-kix_99yhmmrvcmks-8>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-8,decimal) ". "}.lst-kix_99yhmmrvcmks-8>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-8}.lst-kix_jttb5ey0mtdr-1>li:before{content:"-  "}.lst-kix_jttb5ey0mtdr-2>li:before{content:"-  "}.lst-kix_jttb5ey0mtdr-5>li:before{content:"-  "}.lst-kix_jttb5ey0mtdr-6>li:before{content:"-  "}.lst-kix_99yhmmrvcmks-3>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-3,lower-latin) ". "}.lst-kix_99yhmmrvcmks-4>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-4,lower-roman) ". "}.lst-kix_99yhmmrvcmks-5>li:before{content:"" counter(lst-ctn-kix_99yhmmrvcmks-5,decimal) ". "}.lst-kix_kh29jsq0h4s-8>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-8}ol.lst-kix_99yhmmrvcmks-7.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-7 0}ol.lst-kix_xc0vy64nvluu-1.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-1 0}.lst-kix_jttb5ey0mtdr-7>li:before{content:"-  "}.lst-kix_jttb5ey0mtdr-8>li:before{content:"-  "}.lst-kix_dmij6bobu93-0>li{counter-increment:lst-ctn-kix_dmij6bobu93-0}ol.lst-kix_kh29jsq0h4s-4.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-4 0}.lst-kix_vh7w5tnlpfsm-4>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-4}ol.lst-kix_dmij6bobu93-4.start{counter-reset:lst-ctn-kix_dmij6bobu93-4 0}.lst-kix_dmij6bobu93-1>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-1,lower-latin) ". "}.lst-kix_xc0vy64nvluu-3>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-3}.lst-kix_dmij6bobu93-3>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-3,decimal) ". "}.lst-kix_kh29jsq0h4s-4>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-4}ol.lst-kix_a009cfjyoj27-2.start{counter-reset:lst-ctn-kix_a009cfjyoj27-2 0}ol.lst-kix_xc0vy64nvluu-4.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-4 0}.lst-kix_vh7w5tnlpfsm-0>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-0}.lst-kix_hpi6antdus2r-4>li:before{content:"-  "}.lst-kix_51zhi3wgordc-2>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-2,lower-roman) ". "}.lst-kix_51zhi3wgordc-4>li{counter-increment:lst-ctn-kix_51zhi3wgordc-4}ul.lst-kix_4ecl52zhbd6q-8{list-style-type:none}.lst-kix_51zhi3wgordc-0>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-0,decimal) ". "}ul.lst-kix_4ecl52zhbd6q-7{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-4{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-3{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-6{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-5{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-0{list-style-type:none}.lst-kix_hpi6antdus2r-6>li:before{content:"-  "}ul.lst-kix_4ecl52zhbd6q-2{list-style-type:none}ul.lst-kix_4ecl52zhbd6q-1{list-style-type:none}.lst-kix_hpi6antdus2r-8>li:before{content:"-  "}ol.lst-kix_a009cfjyoj27-4.start{counter-reset:lst-ctn-kix_a009cfjyoj27-4 0}ol.lst-kix_vh7w5tnlpfsm-1.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-1 0}.lst-kix_51zhi3wgordc-8>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-8,lower-roman) ". "}.lst-kix_a009cfjyoj27-5>li{counter-increment:lst-ctn-kix_a009cfjyoj27-5}.lst-kix_vh7w5tnlpfsm-7>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-7,lower-latin) ". "}.lst-kix_hpi6antdus2r-0>li:before{content:"-  "}.lst-kix_51zhi3wgordc-4>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-4,lower-latin) ". "}.lst-kix_51zhi3wgordc-6>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-6,decimal) ". "}.lst-kix_u8ma5k7yjif9-7>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-7,lower-latin) ". "}.lst-kix_99yhmmrvcmks-4>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-4}ol.lst-kix_kh29jsq0h4s-7.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-7 0}.lst-kix_hpi6antdus2r-2>li:before{content:"-  "}.lst-kix_u8ma5k7yjif9-8>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-8}.lst-kix_u8ma5k7yjif9-1>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-1,lower-latin) ". "}.lst-kix_u8ma5k7yjif9-3>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-3,decimal) ". "}ol.lst-kix_dmij6bobu93-7.start{counter-reset:lst-ctn-kix_dmij6bobu93-7 0}.lst-kix_vh7w5tnlpfsm-3>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-3,decimal) ". "}.lst-kix_u8ma5k7yjif9-5>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-5,lower-roman) ". "}.lst-kix_vh7w5tnlpfsm-5>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-5,lower-roman) ". "}.lst-kix_dmij6bobu93-5>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-5,lower-roman) ". "}.lst-kix_dloyb1hvabfn-2>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-2}ol.lst-kix_u8ma5k7yjif9-8.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-8 0}.lst-kix_a009cfjyoj27-6>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-6,lower-latin) ". "}.lst-kix_a009cfjyoj27-8>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-8,decimal) ". "}.lst-kix_dmij6bobu93-7>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-7,lower-latin) ". "}.lst-kix_vh7w5tnlpfsm-1>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-1,lower-latin) ". "}ul.lst-kix_hpi6antdus2r-0{list-style-type:none}ol.lst-kix_kh29jsq0h4s-1.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-1 0}.lst-kix_dmij6bobu93-5>li{counter-increment:lst-ctn-kix_dmij6bobu93-5}.lst-kix_a009cfjyoj27-4>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-4,lower-roman) ". "}ol.lst-kix_dmij6bobu93-6.start{counter-reset:lst-ctn-kix_dmij6bobu93-6 0}.lst-kix_99yhmmrvcmks-5>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-5}.lst-kix_a009cfjyoj27-3>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-3,lower-latin) ". "}ul.lst-kix_hpi6antdus2r-7{list-style-type:none}ul.lst-kix_hpi6antdus2r-8{list-style-type:none}ol.lst-kix_dmij6bobu93-0.start{counter-reset:lst-ctn-kix_dmij6bobu93-0 0}ul.lst-kix_hpi6antdus2r-5{list-style-type:none}ul.lst-kix_hpi6antdus2r-6{list-style-type:none}.lst-kix_a009cfjyoj27-0>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-0,lower-latin) ". "}ul.lst-kix_hpi6antdus2r-3{list-style-type:none}ul.lst-kix_hpi6antdus2r-4{list-style-type:none}.lst-kix_51zhi3wgordc-1>li{counter-increment:lst-ctn-kix_51zhi3wgordc-1}ul.lst-kix_hpi6antdus2r-1{list-style-type:none}ul.lst-kix_hpi6antdus2r-2{list-style-type:none}ol.lst-kix_51zhi3wgordc-0{list-style-type:none}ol.lst-kix_a009cfjyoj27-0.start{counter-reset:lst-ctn-kix_a009cfjyoj27-0 0}ol.lst-kix_kh29jsq0h4s-8{list-style-type:none}ol.lst-kix_kh29jsq0h4s-5{list-style-type:none}ol.lst-kix_kh29jsq0h4s-4{list-style-type:none}ol.lst-kix_kh29jsq0h4s-7{list-style-type:none}ol.lst-kix_kh29jsq0h4s-6{list-style-type:none}ol.lst-kix_kh29jsq0h4s-1{list-style-type:none}ol.lst-kix_kh29jsq0h4s-0{list-style-type:none}ol.lst-kix_kh29jsq0h4s-3{list-style-type:none}ol.lst-kix_kh29jsq0h4s-2{list-style-type:none}.lst-kix_dmij6bobu93-7>li{counter-increment:lst-ctn-kix_dmij6bobu93-7}ol.lst-kix_99yhmmrvcmks-5.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-5 0}ol.lst-kix_kh29jsq0h4s-6.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-6 0}.lst-kix_vh7w5tnlpfsm-5>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-5}.lst-kix_u8ma5k7yjif9-3>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-3}ol.lst-kix_dmij6bobu93-2{list-style-type:none}ol.lst-kix_dmij6bobu93-1{list-style-type:none}ol.lst-kix_99yhmmrvcmks-0{list-style-type:none}ol.lst-kix_dmij6bobu93-4{list-style-type:none}ol.lst-kix_99yhmmrvcmks-1{list-style-type:none}ol.lst-kix_dmij6bobu93-3{list-style-type:none}.lst-kix_dloyb1hvabfn-3>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-3}.lst-kix_dloyb1hvabfn-7>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-7,lower-latin) ". "}.lst-kix_51zhi3wgordc-3>li{counter-increment:lst-ctn-kix_51zhi3wgordc-3}ol.lst-kix_dmij6bobu93-0{list-style-type:none}.lst-kix_a009cfjyoj27-6>li{counter-increment:lst-ctn-kix_a009cfjyoj27-6}.lst-kix_u8ma5k7yjif9-7>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-7}ol.lst-kix_dmij6bobu93-6{list-style-type:none}ol.lst-kix_dmij6bobu93-5{list-style-type:none}.lst-kix_dloyb1hvabfn-4>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-4,lower-latin) ". "}ol.lst-kix_dmij6bobu93-8{list-style-type:none}ol.lst-kix_dmij6bobu93-7{list-style-type:none}ol.lst-kix_dmij6bobu93-5.start{counter-reset:lst-ctn-kix_dmij6bobu93-5 0}.lst-kix_xc0vy64nvluu-1>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-1,lower-latin) ". "}.lst-kix_dloyb1hvabfn-8>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-8,lower-roman) ". "}.lst-kix_yq7ycl3ov8vy-7>li:before{content:"-  "}ol.lst-kix_99yhmmrvcmks-0.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-0 0}.lst-kix_vh7w5tnlpfsm-1>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-1}.lst-kix_yq7ycl3ov8vy-6>li:before{content:"-  "}.lst-kix_xc0vy64nvluu-2>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-2,lower-roman) ". "}.lst-kix_dloyb1hvabfn-3>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-3,decimal) ". "}.lst-kix_51zhi3wgordc-6>li{counter-increment:lst-ctn-kix_51zhi3wgordc-6}.lst-kix_xc0vy64nvluu-6>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-6,decimal) ". "}.lst-kix_yq7ycl3ov8vy-3>li:before{content:"-  "}.lst-kix_xc0vy64nvluu-5>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-5,lower-roman) ". "}ol.lst-kix_a009cfjyoj27-1.start{counter-reset:lst-ctn-kix_a009cfjyoj27-1 0}.lst-kix_yq7ycl3ov8vy-2>li:before{content:"-  "}ol.lst-kix_xc0vy64nvluu-8.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-8 0}.lst-kix_dloyb1hvabfn-0>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-0,decimal) ". "}.lst-kix_99yhmmrvcmks-0>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-0}.lst-kix_dmij6bobu93-0>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-0,decimal) ". "}.lst-kix_kh29jsq0h4s-1>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-1,lower-roman) ". "}ol.lst-kix_99yhmmrvcmks-6.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-6 0}.lst-kix_a009cfjyoj27-8>li{counter-increment:lst-ctn-kix_a009cfjyoj27-8}.lst-kix_kh29jsq0h4s-5>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-5,decimal) ". "}ol.lst-kix_dmij6bobu93-2.start{counter-reset:lst-ctn-kix_dmij6bobu93-2 0}.lst-kix_51zhi3wgordc-3>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-3,decimal) ". "}.lst-kix_hpi6antdus2r-5>li:before{content:"-  "}ol.lst-kix_kh29jsq0h4s-0.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-0 0}.lst-kix_u8ma5k7yjif9-0>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-0}ol.lst-kix_kh29jsq0h4s-3.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-3 0}.lst-kix_xc0vy64nvluu-6>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-6}.lst-kix_dmij6bobu93-2>li{counter-increment:lst-ctn-kix_dmij6bobu93-2}ol.lst-kix_dmij6bobu93-1.start{counter-reset:lst-ctn-kix_dmij6bobu93-1 0}ol.lst-kix_u8ma5k7yjif9-0{list-style-type:none}ol.lst-kix_u8ma5k7yjif9-1{list-style-type:none}.lst-kix_vh7w5tnlpfsm-3>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-3}.lst-kix_51zhi3wgordc-7>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-7,lower-latin) ". "}ol.lst-kix_u8ma5k7yjif9-4{list-style-type:none}.lst-kix_vh7w5tnlpfsm-8>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-8,lower-roman) ". "}.lst-kix_51zhi3wgordc-8>li{counter-increment:lst-ctn-kix_51zhi3wgordc-8}ol.lst-kix_u8ma5k7yjif9-5{list-style-type:none}.lst-kix_xc0vy64nvluu-1>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-1}ol.lst-kix_u8ma5k7yjif9-2{list-style-type:none}ol.lst-kix_u8ma5k7yjif9-3{list-style-type:none}.lst-kix_u8ma5k7yjif9-6>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-6,decimal) ". "}.lst-kix_hpi6antdus2r-1>li:before{content:"-  "}ol.lst-kix_99yhmmrvcmks-4.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-4 0}ol.lst-kix_kh29jsq0h4s-2.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-2 0}.lst-kix_u8ma5k7yjif9-5>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-5}.lst-kix_99yhmmrvcmks-7>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-7}.lst-kix_vh7w5tnlpfsm-0>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-0,decimal) ". "}.lst-kix_vh7w5tnlpfsm-4>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-4,lower-latin) ". "}.lst-kix_kh29jsq0h4s-6>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-6}.lst-kix_u8ma5k7yjif9-2>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-2,lower-roman) ". "}ol.lst-kix_u8ma5k7yjif9-8{list-style-type:none}ol.lst-kix_u8ma5k7yjif9-6{list-style-type:none}ol.lst-kix_u8ma5k7yjif9-7{list-style-type:none}.lst-kix_dmij6bobu93-4>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-4,lower-latin) ". "}.lst-kix_dmij6bobu93-8>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-8,lower-roman) ". "}ol.lst-kix_99yhmmrvcmks-3.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-3 0}.lst-kix_a009cfjyoj27-7>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-7,lower-roman) ". "}.lst-kix_dloyb1hvabfn-5>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-5}.lst-kix_a009cfjyoj27-3>li{counter-increment:lst-ctn-kix_a009cfjyoj27-3}.lst-kix_vh7w5tnlpfsm-6>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-6}ol.lst-kix_dmij6bobu93-3.start{counter-reset:lst-ctn-kix_dmij6bobu93-3 0}.lst-kix_dloyb1hvabfn-0>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-0}.lst-kix_4ecl52zhbd6q-0>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-2>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-1>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-4>li:before{content:"-  "}ol.lst-kix_51zhi3wgordc-1.start{counter-reset:lst-ctn-kix_51zhi3wgordc-1 0}.lst-kix_4ecl52zhbd6q-3>li:before{content:"-  "}.lst-kix_51zhi3wgordc-2>li{counter-increment:lst-ctn-kix_51zhi3wgordc-2}ol.lst-kix_u8ma5k7yjif9-0.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-0 0}.lst-kix_u8ma5k7yjif9-6>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-6}ol.lst-kix_xc0vy64nvluu-0.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-0 0}ol.lst-kix_a009cfjyoj27-3.start{counter-reset:lst-ctn-kix_a009cfjyoj27-3 0}ol.lst-kix_99yhmmrvcmks-8.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-8 0}.lst-kix_99yhmmrvcmks-6>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-6}ol.lst-kix_99yhmmrvcmks-1.start{counter-reset:lst-ctn-kix_99yhmmrvcmks-1 0}.lst-kix_4ecl52zhbd6q-8>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-5>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-6>li:before{content:"-  "}.lst-kix_4ecl52zhbd6q-7>li:before{content:"-  "}ol.lst-kix_51zhi3wgordc-6.start{counter-reset:lst-ctn-kix_51zhi3wgordc-6 0}.lst-kix_dmij6bobu93-6>li{counter-increment:lst-ctn-kix_dmij6bobu93-6}ol.lst-kix_vh7w5tnlpfsm-5.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-5 0}.lst-kix_xc0vy64nvluu-8>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-8}ol.lst-kix_u8ma5k7yjif9-5.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-5 0}ol.lst-kix_a009cfjyoj27-8.start{counter-reset:lst-ctn-kix_a009cfjyoj27-8 0}.lst-kix_99yhmmrvcmks-2>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-2}ol.lst-kix_kh29jsq0h4s-8.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-8 0}.lst-kix_kh29jsq0h4s-0>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-0,lower-latin) ". "}.lst-kix_kh29jsq0h4s-2>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-2,decimal) ". "}.lst-kix_51zhi3wgordc-5>li{counter-increment:lst-ctn-kix_51zhi3wgordc-5}.lst-kix_kh29jsq0h4s-6>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-6,lower-latin) ". "}.lst-kix_kh29jsq0h4s-8>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-8,decimal) ". "}ol.lst-kix_dloyb1hvabfn-8.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-8 0}ol.lst-kix_kh29jsq0h4s-5.start{counter-reset:lst-ctn-kix_kh29jsq0h4s-5 0}.lst-kix_kh29jsq0h4s-4>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-4,lower-roman) ". "}ol.lst-kix_xc0vy64nvluu-7.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-7 0}.lst-kix_xc0vy64nvluu-4>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-4}ol.lst-kix_51zhi3wgordc-8.start{counter-reset:lst-ctn-kix_51zhi3wgordc-8 0}ul.lst-kix_jttb5ey0mtdr-8{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-7{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-3.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-3 0}ul.lst-kix_jttb5ey0mtdr-6{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-5{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-4{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-3{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-2{list-style-type:none}.lst-kix_dloyb1hvabfn-7>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-7}ul.lst-kix_jttb5ey0mtdr-1{list-style-type:none}ul.lst-kix_jttb5ey0mtdr-0{list-style-type:none}ol.lst-kix_99yhmmrvcmks-6{list-style-type:none}ol.lst-kix_99yhmmrvcmks-7{list-style-type:none}ol.lst-kix_99yhmmrvcmks-8{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-0.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-0 0}ol.lst-kix_99yhmmrvcmks-2{list-style-type:none}ol.lst-kix_99yhmmrvcmks-3{list-style-type:none}ol.lst-kix_99yhmmrvcmks-4{list-style-type:none}ol.lst-kix_99yhmmrvcmks-5{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-0{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-1{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-2{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-3{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-4{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-5{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-6{list-style-type:none}.lst-kix_99yhmmrvcmks-3>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-3}ol.lst-kix_vh7w5tnlpfsm-7{list-style-type:none}ol.lst-kix_51zhi3wgordc-6{list-style-type:none}ol.lst-kix_51zhi3wgordc-5{list-style-type:none}ol.lst-kix_51zhi3wgordc-8{list-style-type:none}ol.lst-kix_51zhi3wgordc-7{list-style-type:none}ol.lst-kix_51zhi3wgordc-2{list-style-type:none}ol.lst-kix_51zhi3wgordc-1{list-style-type:none}ol.lst-kix_u8ma5k7yjif9-7.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-7 0}ol.lst-kix_51zhi3wgordc-4{list-style-type:none}ol.lst-kix_51zhi3wgordc-3{list-style-type:none}ol.lst-kix_dmij6bobu93-8.start{counter-reset:lst-ctn-kix_dmij6bobu93-8 0}.lst-kix_u8ma5k7yjif9-2>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-2}ol.lst-kix_a009cfjyoj27-5.start{counter-reset:lst-ctn-kix_a009cfjyoj27-5 0}ol.lst-kix_xc0vy64nvluu-5.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-5 0}ol.lst-kix_vh7w5tnlpfsm-8{list-style-type:none}.lst-kix_kh29jsq0h4s-3>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-3}.lst-kix_dloyb1hvabfn-8>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-8}.lst-kix_a009cfjyoj27-5>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-5,decimal) ". "}ol.lst-kix_vh7w5tnlpfsm-2.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-2 0}.lst-kix_a009cfjyoj27-2>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-2,decimal) ". "}.lst-kix_a009cfjyoj27-1>li:before{content:"" counter(lst-ctn-kix_a009cfjyoj27-1,lower-roman) ". "}ol.lst-kix_dloyb1hvabfn-1.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-1 0}.lst-kix_a009cfjyoj27-2>li{counter-increment:lst-ctn-kix_a009cfjyoj27-2}.lst-kix_a009cfjyoj27-4>li{counter-increment:lst-ctn-kix_a009cfjyoj27-4}.lst-kix_dloyb1hvabfn-1>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-1}ol.lst-kix_51zhi3wgordc-4.start{counter-reset:lst-ctn-kix_51zhi3wgordc-4 0}ol.lst-kix_xc0vy64nvluu-3.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-3 0}ol.lst-kix_u8ma5k7yjif9-3.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-3 0}ol.lst-kix_a009cfjyoj27-6.start{counter-reset:lst-ctn-kix_a009cfjyoj27-6 0}.lst-kix_kh29jsq0h4s-2>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-2}ol.lst-kix_xc0vy64nvluu-2.start{counter-reset:lst-ctn-kix_xc0vy64nvluu-2 0}ol.lst-kix_u8ma5k7yjif9-4.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-4 0}.lst-kix_u8ma5k7yjif9-1>li{counter-increment:lst-ctn-kix_u8ma5k7yjif9-1}.lst-kix_dloyb1hvabfn-5>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-5,lower-roman) ". "}.lst-kix_dloyb1hvabfn-6>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-6,decimal) ". "}.lst-kix_xc0vy64nvluu-0>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-0,decimal) ". "}ol.lst-kix_a009cfjyoj27-7.start{counter-reset:lst-ctn-kix_a009cfjyoj27-7 0}ol.lst-kix_dloyb1hvabfn-6.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-6 0}.lst-kix_yq7ycl3ov8vy-8>li:before{content:"-  "}.lst-kix_xc0vy64nvluu-8>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-8,lower-roman) ". "}.lst-kix_dmij6bobu93-3>li{counter-increment:lst-ctn-kix_dmij6bobu93-3}.lst-kix_yq7ycl3ov8vy-4>li:before{content:"-  "}ol.lst-kix_dloyb1hvabfn-0.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-0 0}.lst-kix_xc0vy64nvluu-5>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-5}.lst-kix_yq7ycl3ov8vy-5>li:before{content:"-  "}.lst-kix_vh7w5tnlpfsm-7>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-7}.lst-kix_yq7ycl3ov8vy-0>li:before{content:"-  "}ol.lst-kix_xc0vy64nvluu-8{list-style-type:none}ol.lst-kix_xc0vy64nvluu-7{list-style-type:none}.lst-kix_dloyb1hvabfn-1>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-1,lower-latin) ". "}.lst-kix_dloyb1hvabfn-2>li:before{content:"" counter(lst-ctn-kix_dloyb1hvabfn-2,lower-roman) ". "}.lst-kix_yq7ycl3ov8vy-1>li:before{content:"-  "}.lst-kix_xc0vy64nvluu-3>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-3,decimal) ". "}ol.lst-kix_xc0vy64nvluu-6{list-style-type:none}ol.lst-kix_xc0vy64nvluu-5{list-style-type:none}.lst-kix_xc0vy64nvluu-4>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-4,lower-latin) ". "}ol.lst-kix_xc0vy64nvluu-4{list-style-type:none}ol.lst-kix_xc0vy64nvluu-3{list-style-type:none}ol.lst-kix_dloyb1hvabfn-7.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-7 0}ol.lst-kix_xc0vy64nvluu-2{list-style-type:none}ol.lst-kix_xc0vy64nvluu-1{list-style-type:none}ol.lst-kix_xc0vy64nvluu-0{list-style-type:none}.lst-kix_xc0vy64nvluu-2>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-2}.lst-kix_xc0vy64nvluu-7>li:before{content:"" counter(lst-ctn-kix_xc0vy64nvluu-7,lower-latin) ". "}.lst-kix_a009cfjyoj27-0>li{counter-increment:lst-ctn-kix_a009cfjyoj27-0}ol.lst-kix_dloyb1hvabfn-1{list-style-type:none}ol.lst-kix_dloyb1hvabfn-2{list-style-type:none}ol.lst-kix_dloyb1hvabfn-0{list-style-type:none}ol.lst-kix_dloyb1hvabfn-5{list-style-type:none}ol.lst-kix_dloyb1hvabfn-6{list-style-type:none}ol.lst-kix_dloyb1hvabfn-3{list-style-type:none}ol.lst-kix_dloyb1hvabfn-4{list-style-type:none}.lst-kix_dloyb1hvabfn-6>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-6}.lst-kix_dmij6bobu93-2>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-2,lower-roman) ". "}ol.lst-kix_dloyb1hvabfn-7{list-style-type:none}ol.lst-kix_51zhi3wgordc-5.start{counter-reset:lst-ctn-kix_51zhi3wgordc-5 0}ol.lst-kix_dloyb1hvabfn-8{list-style-type:none}ol.lst-kix_vh7w5tnlpfsm-6.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-6 0}.lst-kix_kh29jsq0h4s-7>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-7,lower-roman) ". "}.lst-kix_kh29jsq0h4s-3>li:before{content:"" counter(lst-ctn-kix_kh29jsq0h4s-3,lower-latin) ". "}ol.lst-kix_u8ma5k7yjif9-2.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-2 0}.lst-kix_kh29jsq0h4s-5>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-5}.lst-kix_99yhmmrvcmks-1>li{counter-increment:lst-ctn-kix_99yhmmrvcmks-1}.lst-kix_51zhi3wgordc-1>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-1,lower-latin) ". "}ol.lst-kix_a009cfjyoj27-1{list-style-type:none}ol.lst-kix_a009cfjyoj27-2{list-style-type:none}.lst-kix_hpi6antdus2r-3>li:before{content:"-  "}ol.lst-kix_a009cfjyoj27-3{list-style-type:none}ol.lst-kix_a009cfjyoj27-4{list-style-type:none}.lst-kix_51zhi3wgordc-7>li{counter-increment:lst-ctn-kix_51zhi3wgordc-7}ol.lst-kix_a009cfjyoj27-0{list-style-type:none}ol.lst-kix_dloyb1hvabfn-5.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-5 0}.lst-kix_hpi6antdus2r-7>li:before{content:"-  "}.lst-kix_a009cfjyoj27-7>li{counter-increment:lst-ctn-kix_a009cfjyoj27-7}ol.lst-kix_a009cfjyoj27-5{list-style-type:none}ol.lst-kix_a009cfjyoj27-6{list-style-type:none}ol.lst-kix_a009cfjyoj27-7{list-style-type:none}ol.lst-kix_a009cfjyoj27-8{list-style-type:none}.lst-kix_dloyb1hvabfn-4>li{counter-increment:lst-ctn-kix_dloyb1hvabfn-4}.lst-kix_dmij6bobu93-1>li{counter-increment:lst-ctn-kix_dmij6bobu93-1}.lst-kix_kh29jsq0h4s-7>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-7}ol.lst-kix_dloyb1hvabfn-2.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-2 0}.lst-kix_vh7w5tnlpfsm-2>li{counter-increment:lst-ctn-kix_vh7w5tnlpfsm-2}.lst-kix_dmij6bobu93-8>li{counter-increment:lst-ctn-kix_dmij6bobu93-8}.lst-kix_xc0vy64nvluu-0>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-0}ol.lst-kix_51zhi3wgordc-3.start{counter-reset:lst-ctn-kix_51zhi3wgordc-3 0}ol.lst-kix_vh7w5tnlpfsm-7.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-7 0}.lst-kix_51zhi3wgordc-5>li:before{content:"" counter(lst-ctn-kix_51zhi3wgordc-5,lower-roman) ". "}.lst-kix_xc0vy64nvluu-7>li{counter-increment:lst-ctn-kix_xc0vy64nvluu-7}.lst-kix_u8ma5k7yjif9-8>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-8,lower-roman) ". "}.lst-kix_vh7w5tnlpfsm-2>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-2,lower-roman) ". "}.lst-kix_vh7w5tnlpfsm-6>li:before{content:"" counter(lst-ctn-kix_vh7w5tnlpfsm-6,decimal) ". "}ol.lst-kix_dloyb1hvabfn-3.start{counter-reset:lst-ctn-kix_dloyb1hvabfn-3 0}ol.lst-kix_u8ma5k7yjif9-1.start{counter-reset:lst-ctn-kix_u8ma5k7yjif9-1 0}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_u8ma5k7yjif9-4>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-4,lower-latin) ". "}.lst-kix_dmij6bobu93-6>li:before{content:"" counter(lst-ctn-kix_dmij6bobu93-6,decimal) ". "}ol.lst-kix_51zhi3wgordc-2.start{counter-reset:lst-ctn-kix_51zhi3wgordc-2 0}.lst-kix_kh29jsq0h4s-0>li{counter-increment:lst-ctn-kix_kh29jsq0h4s-0}ol.lst-kix_vh7w5tnlpfsm-8.start{counter-reset:lst-ctn-kix_vh7w5tnlpfsm-8 0}.lst-kix_u8ma5k7yjif9-0>li:before{content:"" counter(lst-ctn-kix_u8ma5k7yjif9-0,decimal) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c18{margin-left:36pt;padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c22{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c19{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c13{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c10{margin-left:72pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c7{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c12{padding-top:18pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;text-align:left}.c6{margin-left:36pt;padding-top:0pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c9{padding-top:0pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c16{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{padding:0;margin:0}.c4{vertical-align:sub}.c15{font-style:italic}.c23{text-indent:36pt}.c21{height:11pt}.c17{margin-left:108pt}.c20{font-weight:700}.c11{padding-left:0pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16 doc-content"><h2 class="c13" id="h.41u1l6a0sx8f"><span class="c8">Prerequisite</span></h2><p class="c14"><span class="c0">We assume that the reader knows about single agent RL settings with topics covering MDP, bandit setting, Q learning, SARSA etc.</span></p><p class="c2"><span class="c0"></span></p><h2 class="c13" id="h.h6j15wwycgei"><span class="c8">Introduction</span></h2><p class="c14"><span class="c0">In this blog we will talk about multi agent reinforcement learning and some of the concepts associated with it.</span></p><p class="c14"><span>So first what is MARL and how is it different from single agent setting, so MARL consist of an environment with which we interact and some goals like reward maximization and also agents (</span><span class="c20">MULTIPLE</span><span class="c0">),so the difference lies in the number of agents, One can argue that we can look at it as a single agent setting but the joint action space size will increase exponentially as the number of agent increases so we will find some algorithms to solve this problems.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.27px; height: 323.75px;"><img alt="" src="images/image14.png" style="width: 391.27px; height: 323.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><h2 class="c13" id="h.dpyco6bpcb2"><span class="c8">Applications of MARL</span></h2><ol class="c3 lst-kix_51zhi3wgordc-0 start" start="1"><li class="c7 c11 li-bullet-0"><span class="c0">It has been extensively used in warehouse management where multiple robots are deployed to work together.</span></li><li class="c7 c11 li-bullet-0"><span class="c0">Multiple board games can be learned using MARL as we can use multi agents setting where each agent play for itself and exploit weakness of others, resulting in the overall improvement for all the agents</span></li><li class="c7 c11 li-bullet-0"><span class="c0">In trading also it can be used by the same principle as in board games to improve each other to trade better.</span></li></ol><p class="c2"><span class="c0"></span></p><p class="c14"><span class="c0">We will move onto various classes of setting in multi agent reinforcement learning with increasing complexity in hierarchy.</span></p><h2 class="c13" id="h.2ft5mil9bp9l"><span class="c8">Settings in MARL</span></h2><ol class="c3 lst-kix_u8ma5k7yjif9-0 start" start="1"><li class="c18 c11 li-bullet-0"><h3 id="h.dwa56031vu91" style="display:inline"><span class="c19">Normal form games</span></h3></li></ol><p class="c7"><span class="c0">Normal form games are equivalent to bandit setting in single agent RL with only 1 state.</span></p><p class="c7"><span class="c0">Def : It consists of :</span></p><ol class="c3 lst-kix_99yhmmrvcmks-0 start" start="1"><li class="c10 c11 li-bullet-0"><span class="c0">Finite set of agents I</span></li><li class="c10 c11 li-bullet-0"><span class="c0">For each agent i in I</span></li></ol><ul class="c3 lst-kix_jttb5ey0mtdr-0 start"><li class="c14 c11 c17 li-bullet-0"><span>Finite set of action A</span><span class="c4">i</span></li><li class="c14 c17 c11 li-bullet-0"><span>Reward function R</span><span class="c4">i </span><span>with R</span><span class="c4">i</span><span>: A -&gt; R where A = A</span><span class="c4">1</span><span>&nbsp;&times; ... &times; A</span><span class="c4">n</span></li></ul><p class="c14"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normal formal games can be classified on the basis of reward structure as:</span></p><ol class="c3 lst-kix_a009cfjyoj27-0 start" start="1"><li class="c7 c11 li-bullet-0"><span class="c0">Zero-sum game : The sum of agents&#39; reward is always 0 for all joint action A. A game of rock paper scissors could be looked at as a zero sum game.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.67px; height: 29.40px;"><img alt="" src="images/image25.png" style="width: 150.67px; height: 29.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c7 c11 li-bullet-0"><span class="c0">Common reward game: All the agents receive the same reward.</span></li><li class="c7 c11 li-bullet-0"><span class="c0">General-sum game : No constraints</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 76.00px; height: 29.00px;"><img alt="" src="images/image20.png" style="width: 150.00px; height: 29.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><ul class="c3 lst-kix_hpi6antdus2r-0 start"><li class="c7 c11 li-bullet-0"><span class="c0">Normal form games can be extended to repeated normal form game by repeatedly playing the game, now our policy may depend on the history denoted by </span></li></ul><p class="c7"><span class="c0">rest all remains the same.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 124.67px; height: 27.70px;"><img alt="" src="images/image24.png" style="width: 124.67px; height: 27.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 100.67px; height: 25.17px;"><img alt="" src="images/image5.png" style="width: 100.67px; height: 25.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 72.33px; height: 28.38px;"><img alt="" src="images/image18.png" style="width: 72.33px; height: 28.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c22" id="h.dm0qt2w8b6wj"><span class="c19">&nbsp;2. Stochastic Games</span></h3><p class="c14 c23"><span class="c0">Stochastic games are equivalent to MDP for the single RL setting, </span></p><p class="c7"><span class="c0">Def : It consists of :</span></p><ol class="c3 lst-kix_kh29jsq0h4s-0 start" start="1"><li class="c10 c11 li-bullet-0"><span class="c0">Finite set of agents I = {1, ..., n}</span></li><li class="c10 c11 li-bullet-0"><span class="c0">Finite set of states S, with subset of terminal states S&rsquo; &sub; S</span></li><li class="c10 c11 li-bullet-0"><span class="c0">For each agent i &isin; I:</span></li></ol><p class="c10"><span class="c0">&ndash; Finite set of actions Ai</span></p><p class="c10"><span class="c0">&ndash; Reward function Ri : S &times; A &times; S &rarr; R, where A = A1 &times; ... &times; An</span></p><ol class="c3 lst-kix_kh29jsq0h4s-0" start="4"><li class="c10 c11 li-bullet-0"><span class="c0">State transition probability function T : S &times; A &times; S &rarr; [0, 1] such that</span></li></ol><p class="c10"><span>&forall;s &isin; S, a &isin; A : &sum;</span><span class="c4">s&prime;&isin;S</span><span class="c0">T (s, a, s&prime;) = 1 </span></p><ol class="c3 lst-kix_kh29jsq0h4s-0" start="5"><li class="c10 c11 li-bullet-0"><span class="c0">Initial state distribution &mu; : S &rarr; [0, 1] such that</span></li></ol><p class="c10"><span>&sum;</span><span class="c4">s&isin;S</span><span class="c0">&mu;(s) = 1 and &forall;s &isin; S&rsquo; : &mu;(s) = 0</span></p><p class="c2"><span class="c0"></span></p><p class="c14"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So the game starts in some state according to &mu;, and then we select our action based on</span></p><p class="c14"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy &pi;</span><span class="c4">i</span><span>(a</span><span class="c4">t</span><span>i | h</span><span class="c4">t</span><span class="c0">) for the agent i where the history contains all the states and joint actions, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c14"><span class="c0">the property of history is known as full observability and then we transition according to &nbsp;our transition probability function. Also we have markov property as in MDP i.e. </span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.67px; height: 42.30px;"><img alt="" src="images/image28.png" style="width: 538.67px; height: 42.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c7"><span class="c0">One of the examples of stochastic games could be a foraging game where multiple agents have to collect the apples but each agent has a skill level and to collect the apple the skill level of all the agents around an apple should be more than its level. And our task is to collect all the apples as fast as possible.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.00px; height: 297.00px;"><img alt="" src="images/image26.png" style="width: 624.00px; height: 297.00px; margin-left: -61.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c18" id="h.ruqfpqzry3w"><span class="c19">3. Partially observable stochastic game</span></h3><p class="c14"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So it is almost the same as a stochastic game but the property of full observability does not hold and it is more close to the natural setting as in reality many a time agents do not have full information about the state and the action of other agents.</span></p><p class="c14"><span class="c0">Def: It contains all the elements of stochastic games and </span></p><p class="c14"><span>&bull; Finite set of observations </span><span class="c15">O</span><span class="c0">i</span></p><p class="c14"><span>&bull; Observation function O</span><span class="c4">i</span><span>&nbsp;: A &times; S &times; </span><span class="c15">O</span><span class="c4">i</span><span class="c0">&nbsp;&rarr; [0, 1] such that</span></p><p class="c14"><span>&forall;a &isin; A, s &isin; S : &sum;</span><span class="c4">oi&isin;</span><span class="c4 c15">O</span><span class="c4">i</span><span>O</span><span class="c4">i</span><span>(a, s, o</span><span class="c4">i</span><span class="c0">) = 1</span></p><p class="c14"><span>And now the history h</span><span class="c4">t</span><span>=(o</span><span class="c4">1</span><span>,o</span><span class="c4">2</span><span>,....o</span><span class="c4">t</span><span class="c0">)</span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p><p class="c14"><span class="c0">So the whole picture over distinct setting can be presented as </span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.15px; height: 242.67px;"><img alt="" src="images/image10.png" style="width: 417.15px; height: 242.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c0">also we can model communication by extending our existing action space with the help of message space</span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 141.33px; height: 36.87px;"><img alt="" src="images/image16.png" style="width: 141.33px; height: 36.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span>where X</span><span class="c4">i</span><span>&nbsp;is the old action set and M</span><span class="c4">i</span><span class="c0">&nbsp;is the message set</span></p><p class="c9"><span class="c0">And we can also model corruption of messages or the communication error using POSG. Also remember that agents themselves don&rsquo;t understand the meaning of the messages and have to learn it through the process.</span></p><p class="c9 c21"><span class="c0"></span></p><p class="c9"><span class="c0">Now that we have seen the different types of setting in reinforcement learning, we will move to some of the equilibrium concepts where we want our policy to converge.</span></p><h2 class="c13" id="h.q19fq1sd5m60"><span class="c8">Definition for different type of equilibrium</span></h2><p class="c14"><span>(U is the expected discounted reward)</span></p><ol class="c3 lst-kix_dmij6bobu93-0 start" start="1"><li class="c7 c11 li-bullet-0"><span class="c0">Best Response:</span></li></ol><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.60px; height: 27.24px;"><img alt="" src="images/image19.png" style="width: 337.60px; height: 27.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.10px; height: 49.12px;"><img alt="" src="images/image12.png" style="width: 320.10px; height: 49.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c0">So the best response as the name suggest is the best an agent i can do by keeping the policy of other agents fixed.</span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="2"><li class="c7 c11 li-bullet-0"><span class="c0">Minmax solution : In a zero-sum game with two agents, a joint policy &pi; = (&pi;i, &pi;j) is a minimax solution if</span></li></ol><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 194.00px; height: 116.40px;"><img alt="" src="images/image3.png" style="width: 194.00px; height: 116.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="3"><li class="c7 c11 li-bullet-0"><span class="c0">Nash Equilibrium</span></li></ol><p class="c6"><span class="c0">In a general-sum game with n agents, a joint policy &pi; = (&pi;1, ..., &pi;n) is a Nash equilibrium if</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 210.67px; height: 40.38px;"><img alt="" src="images/image22.png" style="width: 210.67px; height: 50.16px; margin-left: 0.00px; margin-top: -9.77px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">So Nash equilibrium says that we can&rsquo;t improve our expected reward unilaterally by just changing our policy.</span></p><p class="c6"><span class="c0">Consider the following example of the game</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 156.60px; height: 93.28px;"><img alt="" src="images/image1.png" style="width: 156.60px; height: 93.28px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span>So here the policy to perform A,A is a Nash equilibrium as if let&#39;s say agent 2 decided to change its policy to improve it will have to play B with some probability and that will reduce the overall reward. Similarly B,B is a nash equilibrium and also if both agents decided to play both A and B with half probability then also it is a nash equilibrium. So there are multiple possible nash equilibriums.</span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="4"><li class="c7 c11 li-bullet-0"><span class="c0">&#1108;-Nash Equilibrium</span></li></ol><p class="c7"><span class="c0">Now we can loosen the constraint imposed by the nash equilibrium by using &#1108;-Nash Equilibrium which says that </span></p><p class="c6"><span class="c0">In a general-sum game with n agents, a joint policy &pi; = (&pi;1, ..., &pi;n) is a &#1108;-Nash equilibrium if</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 224.67px; height: 43.53px;"><img alt="" src="images/image13.png" style="width: 224.67px; height: 43.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">So we are saying that an agent can&rsquo;t improve its own expected reward by more than epsilon by changing its own policy. There is usually no relation between a nash equilibrium and &#1108;-Nash Equilibrium, consider the following example</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 205.15px; height: 102.94px;"><img alt="" src="images/image6.png" style="width: 205.15px; height: 102.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">Here the action A,C is a nash equilibrium but the action B,D is an e-Nash equilibrium with e=1 as the best improvement agent 2 can do is by playing C and increase its award by epsilon.</span></p><p class="c6 c21"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="5"><li class="c7 c11 li-bullet-0"><span class="c0">Correlated Equilibrium</span></li></ol><p class="c7"><span class="c0">Till now we have assumed probabilistically independent policy in nash equilibrium, correlated equilibrium will generalize the nash equilibrium by allowing the correlation between policy.</span></p><p class="c7"><span class="c0">Definition 8 (Correlated equilibrium) In a general-sum normal-form game</span></p><p class="c7"><span class="c0">with n agents, let &pi;c(a) be a joint policy that assigns probabilities to joint</span></p><p class="c7"><span class="c0">actions a &isin; A. Then, &pi;c is a correlated equilibrium if for every agent i &isin; I and</span></p><p class="c7"><span class="c0">every action modifier &xi;i : Ai &rarr; Ai:</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 374.67px; height: 59.78px;"><img alt="" src="images/image11.png" style="width: 374.67px; height: 59.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c0">No agent can deviate from its recommended action to increase its expected reward.</span></p><p class="c7"><span class="c0">Consider the following example</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 235.33px; height: 118.51px;"><img alt="" src="images/image4.png" style="width: 235.33px; height: 118.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 265.33px; height: 74.82px;"><img alt="" src="images/image2.png" style="width: 265.33px; height: 74.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c0">here the first 3 policies are the nash equilibrium and now consider the following policy</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 238.00px; height: 59.91px;"><img alt="" src="images/image8.png" style="width: 238.00px; height: 59.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So if we assume that A got the recommended action as L then its expected reward is 6/2+2/2 = 4, but if it tries to change it to B then it will be reduced to 3.5 and similarly other cases followed so hence its correlated equilibrium.</span></p><p class="c1"><span class="c0"></span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="6"><li class="c7 c11 li-bullet-0"><span class="c0">Pareto Optimality</span></li></ol><p class="c7"><span class="c0">Now as we have seen that there are multiple possible equillibrium so to put more constraint on the equilibrium we defines this,</span></p><p class="c6"><span>Def: A joint policy &pi; is Pareto- dominated by another joint policy &pi;&prime; if</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.50px; height: 45.09px;"><img alt="" src="images/image9.png" style="width: 389.50px; height: 45.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c0">So a policy is pareto efficient if it is not dominated by other policies.</span></p><p class="c2"><span class="c0"></span></p><ol class="c3 lst-kix_dmij6bobu93-0" start="7"><li class="c7 c11 li-bullet-0"><span class="c0">No-regret</span></li></ol><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 289.33px; height: 59.00px;"><img alt="" src="images/image17.png" style="width: 289.33px; height: 59.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c0">An agent is said to have no-regret if its average regret in the limit of z &rarr; &infin; is at most zero. No-regret necessitates that every agent in the game have no-regret as a solution notion.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 258.00px; height: 53.17px;"><img alt="" src="images/image7.png" style="width: 258.00px; height: 53.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c21"><span class="c0"></span></p><p class="c9"><span class="c0">Now we have seen various solution concepts and now we will finally move onto some of the algorithms for solving MARL.</span></p><p class="c9"><span class="c0">We will look into single agent RL reduction problems and directly apply Q learning.</span></p><ol class="c3 lst-kix_vh7w5tnlpfsm-0 start" start="1"><li class="c5 li-bullet-0"><span class="c0">Central Q learning</span></li></ol><p class="c6"><span class="c0">In this we reduce multiple agents into a single agent and then learn the policy, it suffers from large action space. And the second limitation is that in most of the MARL setting there is no central unit which guides all the agents so we have to learn an independent policy. And also here we need a notion of scalar reward r instead of individual reward which environment gives.</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 298.67px;"><img alt="" src="images/image23.png" style="width: 624.00px; height: 298.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c3 lst-kix_vh7w5tnlpfsm-0" start="2"><li class="c5 li-bullet-0"><span class="c0">Independent Q learning</span></li></ol><p class="c6"><span class="c0">In this each of the agents work on their own using Q learning algorithms and also agents don&rsquo;t use other agents&#39; action and observation in deciding their own policy. It suffers from instability as the transition probability for an agent i also depend on the other agents policy as </span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 473.33px; height: 69.03px;"><img alt="" src="images/image15.png" style="width: 473.33px; height: 69.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">So for agent i this creates instability.</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 325.33px;"><img alt="" src="images/image27.png" style="width: 624.00px; height: 325.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c0">Here is a comparison for CQl vs IQL on foraging task, we see IQL converge faster due to less size of action space</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 609.57px; height: 352.13px;"><img alt="" src="images/image21.png" style="width: 609.57px; height: 352.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">\</span></p><h2 class="c12" id="h.p2qgj3glnc5"><span class="c8">Challenges in MARL</span></h2><ol class="c3 lst-kix_xc0vy64nvluu-0 start" start="1"><li class="c7 c11 li-bullet-0"><span class="c0">Non stationarity</span></li></ol><p class="c7"><span>As we have seen in the case of </span><span>IQL that</span><span class="c0">&nbsp;MARL suffers from non-stationarity and it is true even for single agent RL setting.</span></p><ol class="c3 lst-kix_xc0vy64nvluu-0" start="2"><li class="c7 c11 li-bullet-0"><span class="c0">Optimality of policies</span></li></ol><p class="c7"><span class="c0">The optimality of policy does not guarantee the best reward we can get for example as we have seen even if we converge to a nash equilibrium it might be possibl;e that some other policy gives better reward.</span></p><ol class="c3 lst-kix_xc0vy64nvluu-0" start="3"><li class="c7 c11 li-bullet-0"><span class="c0">Equilibrium Selection</span></li></ol><p class="c7"><span class="c0">Also we have seen that from a particular kind of optimality there might be multiple equilibrium possible and it is hard to decide on which one to converge.</span></p><ol class="c3 lst-kix_xc0vy64nvluu-0" start="4"><li class="c7 c11 li-bullet-0"><span class="c0">Credit system</span></li></ol><p class="c7"><span class="c0">In the case of MARL, sometimes the success is due to the actions of multiple agents and it becomes hard to decide how to distribute rewards among them.</span></p><ol class="c3 lst-kix_xc0vy64nvluu-0" start="5"><li class="c7 c11 li-bullet-0"><span class="c0">Scaling</span></li></ol><p class="c7"><span class="c0">We have seen in the case of CQL that the actionspace increases exponentially as the number of agents increases.</span></p><p class="c9 c21"><span class="c0"></span></p><p class="c6 c21"><span class="c0"></span></p><p class="c9 c21"><span class="c0"></span></p><p class="c9 c21"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p></body></html>