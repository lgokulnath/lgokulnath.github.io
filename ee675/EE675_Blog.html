<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=wJuDEFlUsDZt7lpAAPdLm3n3ehRllhl-Wd87cCwaxvU6hoH7iC0kvCd7r1OtU9I8aG446YNqTpZd-_Ge-yVN-g);.lst-kix_list_14-1>li:before{content:"" counter(lst-ctn-kix_list_14-1,lower-roman) ". "}.lst-kix_list_14-3>li:before{content:"" counter(lst-ctn-kix_list_14-3,lower-latin) ". "}ol.lst-kix_list_15-6{list-style-type:none}ol.lst-kix_list_15-7{list-style-type:none}.lst-kix_list_21-8>li{counter-increment:lst-ctn-kix_list_21-8}ol.lst-kix_list_15-8{list-style-type:none}ol.lst-kix_list_9-0.start{counter-reset:lst-ctn-kix_list_9-0 0}.lst-kix_list_14-0>li:before{content:"" counter(lst-ctn-kix_list_14-0,lower-latin) ". "}.lst-kix_list_14-4>li:before{content:"" counter(lst-ctn-kix_list_14-4,lower-roman) ". "}ol.lst-kix_list_15-2{list-style-type:none}ol.lst-kix_list_15-3{list-style-type:none}ol.lst-kix_list_15-4{list-style-type:none}.lst-kix_list_14-5>li:before{content:"" counter(lst-ctn-kix_list_14-5,decimal) ". "}.lst-kix_list_14-7>li:before{content:"" counter(lst-ctn-kix_list_14-7,lower-roman) ". "}ol.lst-kix_list_15-5{list-style-type:none}ol.lst-kix_list_15-0{list-style-type:none}ol.lst-kix_list_13-4.start{counter-reset:lst-ctn-kix_list_13-4 0}.lst-kix_list_14-6>li:before{content:"" counter(lst-ctn-kix_list_14-6,lower-latin) ". "}ol.lst-kix_list_15-1{list-style-type:none}.lst-kix_list_13-0>li{counter-increment:lst-ctn-kix_list_13-0}ol.lst-kix_list_20-2.start{counter-reset:lst-ctn-kix_list_20-2 0}.lst-kix_list_17-0>li{counter-increment:lst-ctn-kix_list_17-0}.lst-kix_list_9-0>li{counter-increment:lst-ctn-kix_list_9-0}ol.lst-kix_list_11-8.start{counter-reset:lst-ctn-kix_list_11-8 0}.lst-kix_list_14-2>li:before{content:"" counter(lst-ctn-kix_list_14-2,decimal) ". "}.lst-kix_list_20-7>li{counter-increment:lst-ctn-kix_list_20-7}ol.lst-kix_list_12-0.start{counter-reset:lst-ctn-kix_list_12-0 0}ol.lst-kix_list_17-1.start{counter-reset:lst-ctn-kix_list_17-1 0}ol.lst-kix_list_21-6.start{counter-reset:lst-ctn-kix_list_21-6 0}.lst-kix_list_14-8>li:before{content:"" counter(lst-ctn-kix_list_14-8,decimal) ". "}ol.lst-kix_list_15-5.start{counter-reset:lst-ctn-kix_list_15-5 0}ol.lst-kix_list_10-4.start{counter-reset:lst-ctn-kix_list_10-4 0}.lst-kix_list_5-0>li:before{content:"\0025cf   "}.lst-kix_list_14-8>li{counter-increment:lst-ctn-kix_list_14-8}.lst-kix_list_5-3>li:before{content:"\0025cf   "}ol.lst-kix_list_9-5.start{counter-reset:lst-ctn-kix_list_9-5 0}.lst-kix_list_5-2>li:before{content:"\0025a0   "}.lst-kix_list_5-1>li:before{content:"\0025cb   "}.lst-kix_list_5-7>li:before{content:"\0025cb   "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}ol.lst-kix_list_20-7.start{counter-reset:lst-ctn-kix_list_20-7 0}.lst-kix_list_5-6>li:before{content:"\0025cf   "}.lst-kix_list_5-8>li:before{content:"\0025a0   "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_9-4>li{counter-increment:lst-ctn-kix_list_9-4}.lst-kix_list_5-4>li:before{content:"\0025cb   "}.lst-kix_list_5-5>li:before{content:"\0025a0   "}ul.lst-kix_list_8-0{list-style-type:none}ul.lst-kix_list_8-1{list-style-type:none}ol.lst-kix_list_14-1.start{counter-reset:lst-ctn-kix_list_14-1 0}ol.lst-kix_list_12-5.start{counter-reset:lst-ctn-kix_list_12-5 0}.lst-kix_list_6-1>li:before{content:"-  "}.lst-kix_list_6-3>li:before{content:"-  "}.lst-kix_list_6-0>li:before{content:"-  "}.lst-kix_list_6-4>li:before{content:"-  "}ol.lst-kix_list_14-8.start{counter-reset:lst-ctn-kix_list_14-8 0}ul.lst-kix_list_16-2{list-style-type:none}ul.lst-kix_list_16-1{list-style-type:none}ul.lst-kix_list_16-0{list-style-type:none}.lst-kix_list_6-2>li:before{content:"-  "}ol.lst-kix_list_15-0.start{counter-reset:lst-ctn-kix_list_15-0 0}ul.lst-kix_list_16-8{list-style-type:none}ul.lst-kix_list_16-7{list-style-type:none}ul.lst-kix_list_16-6{list-style-type:none}ul.lst-kix_list_16-5{list-style-type:none}ul.lst-kix_list_16-4{list-style-type:none}.lst-kix_list_6-8>li:before{content:"-  "}ul.lst-kix_list_16-3{list-style-type:none}.lst-kix_list_6-5>li:before{content:"-  "}.lst-kix_list_6-7>li:before{content:"-  "}.lst-kix_list_6-6>li:before{content:"-  "}ol.lst-kix_list_17-8{list-style-type:none}ol.lst-kix_list_10-6.start{counter-reset:lst-ctn-kix_list_10-6 0}.lst-kix_list_7-4>li:before{content:"-  "}.lst-kix_list_7-6>li:before{content:"-  "}ol.lst-kix_list_17-4{list-style-type:none}ol.lst-kix_list_19-7.start{counter-reset:lst-ctn-kix_list_19-7 0}ol.lst-kix_list_17-5{list-style-type:none}.lst-kix_list_15-5>li{counter-increment:lst-ctn-kix_list_15-5}ol.lst-kix_list_17-6{list-style-type:none}.lst-kix_list_22-2>li:before{content:"\0025aa   "}.lst-kix_list_22-6>li:before{content:"\0025aa   "}ol.lst-kix_list_17-7{list-style-type:none}ol.lst-kix_list_17-0{list-style-type:none}ol.lst-kix_list_17-1{list-style-type:none}ol.lst-kix_list_17-2{list-style-type:none}.lst-kix_list_7-2>li:before{content:"-  "}ol.lst-kix_list_17-3{list-style-type:none}.lst-kix_list_22-0>li:before{content:"\0025cf   "}.lst-kix_list_22-8>li:before{content:"\0025aa   "}.lst-kix_list_12-6>li{counter-increment:lst-ctn-kix_list_12-6}ol.lst-kix_list_9-7{list-style-type:none}ol.lst-kix_list_9-8{list-style-type:none}.lst-kix_list_13-7>li:before{content:"" counter(lst-ctn-kix_list_13-7,lower-latin) ". "}ol.lst-kix_list_9-3{list-style-type:none}ol.lst-kix_list_9-4{list-style-type:none}ol.lst-kix_list_9-5{list-style-type:none}.lst-kix_list_7-8>li:before{content:"-  "}ol.lst-kix_list_9-6{list-style-type:none}.lst-kix_list_15-6>li{counter-increment:lst-ctn-kix_list_15-6}ol.lst-kix_list_9-0{list-style-type:none}ol.lst-kix_list_9-1{list-style-type:none}ol.lst-kix_list_9-2{list-style-type:none}.lst-kix_list_22-4>li:before{content:"\0025aa   "}.lst-kix_list_15-5>li:before{content:"" counter(lst-ctn-kix_list_15-5,decimal) ". "}.lst-kix_list_9-8>li{counter-increment:lst-ctn-kix_list_9-8}.lst-kix_list_13-4>li{counter-increment:lst-ctn-kix_list_13-4}.lst-kix_list_4-1>li:before{content:"\0025cb   "}.lst-kix_list_15-7>li:before{content:"" counter(lst-ctn-kix_list_15-7,lower-roman) ". "}.lst-kix_list_17-7>li{counter-increment:lst-ctn-kix_list_17-7}.lst-kix_list_4-3>li:before{content:"\0025cf   "}.lst-kix_list_4-5>li:before{content:"\0025a0   "}.lst-kix_list_10-5>li{counter-increment:lst-ctn-kix_list_10-5}.lst-kix_list_15-1>li:before{content:"" counter(lst-ctn-kix_list_15-1,lower-roman) ". "}.lst-kix_list_15-3>li:before{content:"" counter(lst-ctn-kix_list_15-3,lower-latin) ". "}ol.lst-kix_list_9-2.start{counter-reset:lst-ctn-kix_list_9-2 0}.lst-kix_list_20-0>li{counter-increment:lst-ctn-kix_list_20-0}.lst-kix_list_9-3>li{counter-increment:lst-ctn-kix_list_9-3}.lst-kix_list_11-2>li{counter-increment:lst-ctn-kix_list_11-2}ol.lst-kix_list_15-2.start{counter-reset:lst-ctn-kix_list_15-2 0}.lst-kix_list_19-2>li{counter-increment:lst-ctn-kix_list_19-2}.lst-kix_list_12-3>li:before{content:"" counter(lst-ctn-kix_list_12-3,lower-latin) ". "}.lst-kix_list_12-1>li:before{content:"" counter(lst-ctn-kix_list_12-1,lower-roman) ". "}.lst-kix_list_13-3>li{counter-increment:lst-ctn-kix_list_13-3}ol.lst-kix_list_13-6.start{counter-reset:lst-ctn-kix_list_13-6 0}.lst-kix_list_10-4>li{counter-increment:lst-ctn-kix_list_10-4}ol.lst-kix_list_15-3.start{counter-reset:lst-ctn-kix_list_15-3 0}.lst-kix_list_14-1>li{counter-increment:lst-ctn-kix_list_14-1}.lst-kix_list_21-4>li{counter-increment:lst-ctn-kix_list_21-4}.lst-kix_list_13-3>li:before{content:"" counter(lst-ctn-kix_list_13-3,decimal) ". "}ul.lst-kix_list_18-0{list-style-type:none}.lst-kix_list_13-5>li:before{content:"" counter(lst-ctn-kix_list_13-5,lower-roman) ". "}.lst-kix_list_12-5>li:before{content:"" counter(lst-ctn-kix_list_12-5,decimal) ". "}ul.lst-kix_list_18-8{list-style-type:none}ol.lst-kix_list_13-7.start{counter-reset:lst-ctn-kix_list_13-7 0}ul.lst-kix_list_18-7{list-style-type:none}ul.lst-kix_list_18-6{list-style-type:none}ol.lst-kix_list_12-2.start{counter-reset:lst-ctn-kix_list_12-2 0}ul.lst-kix_list_18-5{list-style-type:none}.lst-kix_list_12-7>li:before{content:"" counter(lst-ctn-kix_list_12-7,lower-roman) ". "}ul.lst-kix_list_18-4{list-style-type:none}ul.lst-kix_list_18-3{list-style-type:none}ul.lst-kix_list_18-2{list-style-type:none}ol.lst-kix_list_21-1.start{counter-reset:lst-ctn-kix_list_21-1 0}ul.lst-kix_list_18-1{list-style-type:none}.lst-kix_list_13-1>li:before{content:"" counter(lst-ctn-kix_list_13-1,lower-latin) ". "}ol.lst-kix_list_19-0.start{counter-reset:lst-ctn-kix_list_19-0 0}ol.lst-kix_list_21-3.start{counter-reset:lst-ctn-kix_list_21-3 0}ol.lst-kix_list_11-6{list-style-type:none}ol.lst-kix_list_11-7{list-style-type:none}ol.lst-kix_list_11-8{list-style-type:none}ol.lst-kix_list_11-2{list-style-type:none}.lst-kix_list_3-0>li:before{content:"-  "}ol.lst-kix_list_11-3{list-style-type:none}ol.lst-kix_list_11-4{list-style-type:none}ol.lst-kix_list_11-5{list-style-type:none}ol.lst-kix_list_20-5.start{counter-reset:lst-ctn-kix_list_20-5 0}ul.lst-kix_list_5-7{list-style-type:none}ol.lst-kix_list_13-1.start{counter-reset:lst-ctn-kix_list_13-1 0}ul.lst-kix_list_5-8{list-style-type:none}ul.lst-kix_list_5-5{list-style-type:none}ol.lst-kix_list_11-0{list-style-type:none}ul.lst-kix_list_5-6{list-style-type:none}ol.lst-kix_list_11-1{list-style-type:none}.lst-kix_list_21-8>li:before{content:"" counter(lst-ctn-kix_list_21-8,lower-roman) ". "}ul.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_3-4>li:before{content:"-  "}.lst-kix_list_10-0>li{counter-increment:lst-ctn-kix_list_10-0}ul.lst-kix_list_5-3{list-style-type:none}.lst-kix_list_3-3>li:before{content:"-  "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}.lst-kix_list_8-0>li:before{content:"-  "}ul.lst-kix_list_5-2{list-style-type:none}.lst-kix_list_8-7>li:before{content:"-  "}.lst-kix_list_3-8>li:before{content:"-  "}.lst-kix_list_21-0>li:before{content:"" counter(lst-ctn-kix_list_21-0,decimal) ". "}ol.lst-kix_list_10-7.start{counter-reset:lst-ctn-kix_list_10-7 0}.lst-kix_list_13-1>li{counter-increment:lst-ctn-kix_list_13-1}.lst-kix_list_21-1>li:before{content:"" counter(lst-ctn-kix_list_21-1,lower-latin) ". "}ol.lst-kix_list_15-8.start{counter-reset:lst-ctn-kix_list_15-8 0}.lst-kix_list_8-3>li:before{content:"-  "}.lst-kix_list_3-7>li:before{content:"-  "}.lst-kix_list_8-4>li:before{content:"-  "}.lst-kix_list_19-1>li{counter-increment:lst-ctn-kix_list_19-1}.lst-kix_list_10-2>li{counter-increment:lst-ctn-kix_list_10-2}.lst-kix_list_17-1>li{counter-increment:lst-ctn-kix_list_17-1}.lst-kix_list_11-1>li:before{content:"" counter(lst-ctn-kix_list_11-1,lower-latin) ". "}.lst-kix_list_21-5>li:before{content:"" counter(lst-ctn-kix_list_21-5,lower-roman) ". "}.lst-kix_list_21-4>li:before{content:"" counter(lst-ctn-kix_list_21-4,lower-latin) ". "}.lst-kix_list_11-0>li:before{content:"" counter(lst-ctn-kix_list_11-0,decimal) ". "}ol.lst-kix_list_9-3.start{counter-reset:lst-ctn-kix_list_9-3 0}.lst-kix_list_8-8>li:before{content:"-  "}.lst-kix_list_16-8>li:before{content:"-  "}.lst-kix_list_16-7>li:before{content:"-  "}.lst-kix_list_17-8>li{counter-increment:lst-ctn-kix_list_17-8}ol.lst-kix_list_19-5.start{counter-reset:lst-ctn-kix_list_19-5 0}.lst-kix_list_4-8>li:before{content:"\0025a0   "}ol.lst-kix_list_12-5{list-style-type:none}ol.lst-kix_list_12-6{list-style-type:none}.lst-kix_list_21-5>li{counter-increment:lst-ctn-kix_list_21-5}.lst-kix_list_4-7>li:before{content:"\0025cb   "}ol.lst-kix_list_12-7{list-style-type:none}.lst-kix_list_14-2>li{counter-increment:lst-ctn-kix_list_14-2}ol.lst-kix_list_20-0.start{counter-reset:lst-ctn-kix_list_20-0 0}.lst-kix_list_17-0>li:before{content:"" counter(lst-ctn-kix_list_17-0,decimal) ". "}ol.lst-kix_list_12-8{list-style-type:none}ol.lst-kix_list_12-1{list-style-type:none}ol.lst-kix_list_12-2{list-style-type:none}ol.lst-kix_list_12-3{list-style-type:none}ol.lst-kix_list_12-4{list-style-type:none}ul.lst-kix_list_4-8{list-style-type:none}.lst-kix_list_16-0>li:before{content:"-  "}ul.lst-kix_list_4-6{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ol.lst-kix_list_12-0{list-style-type:none}.lst-kix_list_21-0>li{counter-increment:lst-ctn-kix_list_21-0}ul.lst-kix_list_4-0{list-style-type:none}.lst-kix_list_16-4>li:before{content:"-  "}ul.lst-kix_list_4-1{list-style-type:none}ol.lst-kix_list_10-8.start{counter-reset:lst-ctn-kix_list_10-8 0}.lst-kix_list_16-3>li:before{content:"-  "}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}.lst-kix_list_11-3>li{counter-increment:lst-ctn-kix_list_11-3}.lst-kix_list_17-7>li:before{content:"" counter(lst-ctn-kix_list_17-7,lower-latin) ". "}ol.lst-kix_list_21-4.start{counter-reset:lst-ctn-kix_list_21-4 0}.lst-kix_list_17-8>li:before{content:"" counter(lst-ctn-kix_list_17-8,lower-roman) ". "}.lst-kix_list_17-3>li:before{content:"" counter(lst-ctn-kix_list_17-3,decimal) ". "}.lst-kix_list_17-4>li:before{content:"" counter(lst-ctn-kix_list_17-4,lower-latin) ". "}ol.lst-kix_list_20-6.start{counter-reset:lst-ctn-kix_list_20-6 0}.lst-kix_list_7-0>li:before{content:"-  "}.lst-kix_list_13-8>li{counter-increment:lst-ctn-kix_list_13-8}ol.lst-kix_list_19-6.start{counter-reset:lst-ctn-kix_list_19-6 0}ol.lst-kix_list_9-7.start{counter-reset:lst-ctn-kix_list_9-7 0}.lst-kix_list_22-5>li:before{content:"\0025aa   "}ol.lst-kix_list_13-8{list-style-type:none}.lst-kix_list_2-4>li:before{content:"-  "}.lst-kix_list_2-8>li:before{content:"-  "}ol.lst-kix_list_13-4{list-style-type:none}ol.lst-kix_list_13-5{list-style-type:none}.lst-kix_list_21-2>li{counter-increment:lst-ctn-kix_list_21-2}ol.lst-kix_list_13-6{list-style-type:none}ol.lst-kix_list_13-7{list-style-type:none}.lst-kix_list_22-1>li:before{content:"o  "}ol.lst-kix_list_13-0{list-style-type:none}ol.lst-kix_list_13-1{list-style-type:none}.lst-kix_list_20-2>li{counter-increment:lst-ctn-kix_list_20-2}ol.lst-kix_list_13-2{list-style-type:none}ol.lst-kix_list_15-7.start{counter-reset:lst-ctn-kix_list_15-7 0}.lst-kix_list_7-3>li:before{content:"-  "}ol.lst-kix_list_13-3{list-style-type:none}ul.lst-kix_list_7-5{list-style-type:none}ul.lst-kix_list_7-6{list-style-type:none}.lst-kix_list_10-0>li:before{content:"" counter(lst-ctn-kix_list_10-0,decimal) ". "}ul.lst-kix_list_7-3{list-style-type:none}.lst-kix_list_9-7>li{counter-increment:lst-ctn-kix_list_9-7}ol.lst-kix_list_21-7.start{counter-reset:lst-ctn-kix_list_21-7 0}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_13-6>li{counter-increment:lst-ctn-kix_list_13-6}.lst-kix_list_13-8>li:before{content:"" counter(lst-ctn-kix_list_13-8,lower-roman) ". "}ol.lst-kix_list_14-6.start{counter-reset:lst-ctn-kix_list_14-6 0}.lst-kix_list_18-3>li:before{content:"-  "}.lst-kix_list_18-7>li:before{content:"-  "}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ul.lst-kix_list_7-1{list-style-type:none}.lst-kix_list_19-6>li{counter-increment:lst-ctn-kix_list_19-6}ul.lst-kix_list_7-2{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"-  "}.lst-kix_list_20-4>li{counter-increment:lst-ctn-kix_list_20-4}.lst-kix_list_15-4>li:before{content:"" counter(lst-ctn-kix_list_15-4,lower-roman) ". "}.lst-kix_list_9-5>li{counter-increment:lst-ctn-kix_list_9-5}ol.lst-kix_list_19-1.start{counter-reset:lst-ctn-kix_list_19-1 0}.lst-kix_list_10-4>li:before{content:"" counter(lst-ctn-kix_list_10-4,lower-latin) ". "}.lst-kix_list_10-8>li:before{content:"" counter(lst-ctn-kix_list_10-8,lower-roman) ". "}ol.lst-kix_list_20-4.start{counter-reset:lst-ctn-kix_list_20-4 0}.lst-kix_list_4-0>li:before{content:"\0025cf   "}.lst-kix_list_15-0>li:before{content:"" counter(lst-ctn-kix_list_15-0,lower-latin) ". "}.lst-kix_list_15-8>li:before{content:"" counter(lst-ctn-kix_list_15-8,decimal) ". "}ol.lst-kix_list_19-4.start{counter-reset:lst-ctn-kix_list_19-4 0}ol.lst-kix_list_14-3.start{counter-reset:lst-ctn-kix_list_14-3 0}.lst-kix_list_15-7>li{counter-increment:lst-ctn-kix_list_15-7}.lst-kix_list_4-4>li:before{content:"\0025cb   "}ol.lst-kix_list_20-1.start{counter-reset:lst-ctn-kix_list_20-1 0}.lst-kix_list_19-4>li{counter-increment:lst-ctn-kix_list_19-4}.lst-kix_list_9-3>li:before{content:"" counter(lst-ctn-kix_list_9-3,decimal) ". "}.lst-kix_list_12-8>li{counter-increment:lst-ctn-kix_list_12-8}ol.lst-kix_list_13-2.start{counter-reset:lst-ctn-kix_list_13-2 0}ol.lst-kix_list_14-7{list-style-type:none}ol.lst-kix_list_14-4.start{counter-reset:lst-ctn-kix_list_14-4 0}ol.lst-kix_list_14-8{list-style-type:none}.lst-kix_list_9-7>li:before{content:"" counter(lst-ctn-kix_list_9-7,lower-latin) ". "}ol.lst-kix_list_14-3{list-style-type:none}ol.lst-kix_list_21-8.start{counter-reset:lst-ctn-kix_list_21-8 0}ol.lst-kix_list_14-4{list-style-type:none}ol.lst-kix_list_14-5{list-style-type:none}ol.lst-kix_list_14-6{list-style-type:none}.lst-kix_list_11-4>li:before{content:"" counter(lst-ctn-kix_list_11-4,lower-latin) ". "}.lst-kix_list_15-2>li{counter-increment:lst-ctn-kix_list_15-2}ol.lst-kix_list_14-0{list-style-type:none}.lst-kix_list_12-4>li:before{content:"" counter(lst-ctn-kix_list_12-4,lower-roman) ". "}ol.lst-kix_list_14-1{list-style-type:none}ol.lst-kix_list_14-2{list-style-type:none}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}.lst-kix_list_20-5>li:before{content:"" counter(lst-ctn-kix_list_20-5,lower-roman) ". "}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_1-0>li:before{content:"-  "}ol.lst-kix_list_19-2.start{counter-reset:lst-ctn-kix_list_19-2 0}.lst-kix_list_20-1>li:before{content:"" counter(lst-ctn-kix_list_20-1,lower-latin) ". "}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_11-8>li:before{content:"" counter(lst-ctn-kix_list_11-8,lower-roman) ". "}.lst-kix_list_12-3>li{counter-increment:lst-ctn-kix_list_12-3}ul.lst-kix_list_6-3{list-style-type:none}ul.lst-kix_list_6-0{list-style-type:none}.lst-kix_list_12-0>li:before{content:"" counter(lst-ctn-kix_list_12-0,lower-latin) ". "}.lst-kix_list_17-3>li{counter-increment:lst-ctn-kix_list_17-3}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"-  "}.lst-kix_list_13-0>li:before{content:"" counter(lst-ctn-kix_list_13-0,decimal) ". "}.lst-kix_list_21-7>li{counter-increment:lst-ctn-kix_list_21-7}ol.lst-kix_list_13-0.start{counter-reset:lst-ctn-kix_list_13-0 0}.lst-kix_list_14-4>li{counter-increment:lst-ctn-kix_list_14-4}.lst-kix_list_13-4>li:before{content:"" counter(lst-ctn-kix_list_13-4,lower-latin) ". "}.lst-kix_list_10-7>li{counter-increment:lst-ctn-kix_list_10-7}ol.lst-kix_list_19-3.start{counter-reset:lst-ctn-kix_list_19-3 0}.lst-kix_list_2-0>li:before{content:"-  "}.lst-kix_list_11-5>li{counter-increment:lst-ctn-kix_list_11-5}ol.lst-kix_list_14-5.start{counter-reset:lst-ctn-kix_list_14-5 0}ol.lst-kix_list_9-8.start{counter-reset:lst-ctn-kix_list_9-8 0}.lst-kix_list_1-8>li:before{content:"-  "}.lst-kix_list_12-8>li:before{content:"" counter(lst-ctn-kix_list_12-8,decimal) ". "}ol.lst-kix_list_20-3.start{counter-reset:lst-ctn-kix_list_20-3 0}.lst-kix_list_19-0>li:before{content:"" counter(lst-ctn-kix_list_19-0,decimal) ". "}.lst-kix_list_19-1>li:before{content:"" counter(lst-ctn-kix_list_19-1,lower-latin) ". "}ol.lst-kix_list_17-7.start{counter-reset:lst-ctn-kix_list_17-7 0}ul.lst-kix_list_1-0{list-style-type:none}ol.lst-kix_list_12-6.start{counter-reset:lst-ctn-kix_list_12-6 0}.lst-kix_list_19-4>li:before{content:"" counter(lst-ctn-kix_list_19-4,lower-latin) ". "}.lst-kix_list_19-2>li:before{content:"" counter(lst-ctn-kix_list_19-2,lower-roman) ". "}.lst-kix_list_19-3>li:before{content:"" counter(lst-ctn-kix_list_19-3,decimal) ". "}ol.lst-kix_list_21-0.start{counter-reset:lst-ctn-kix_list_21-0 0}.lst-kix_list_15-0>li{counter-increment:lst-ctn-kix_list_15-0}ul.lst-kix_list_1-3{list-style-type:none}.lst-kix_list_11-0>li{counter-increment:lst-ctn-kix_list_11-0}ul.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_19-0>li{counter-increment:lst-ctn-kix_list_19-0}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_9-6.start{counter-reset:lst-ctn-kix_list_9-6 0}ol.lst-kix_list_19-8.start{counter-reset:lst-ctn-kix_list_19-8 0}.lst-kix_list_19-8>li:before{content:"" counter(lst-ctn-kix_list_19-8,lower-roman) ". "}ol.lst-kix_list_14-7.start{counter-reset:lst-ctn-kix_list_14-7 0}ol.lst-kix_list_20-8.start{counter-reset:lst-ctn-kix_list_20-8 0}ol.lst-kix_list_11-2.start{counter-reset:lst-ctn-kix_list_11-2 0}.lst-kix_list_19-5>li:before{content:"" counter(lst-ctn-kix_list_19-5,lower-roman) ". "}.lst-kix_list_19-6>li:before{content:"" counter(lst-ctn-kix_list_19-6,decimal) ". "}.lst-kix_list_19-7>li:before{content:"" counter(lst-ctn-kix_list_19-7,lower-latin) ". "}.lst-kix_list_9-2>li{counter-increment:lst-ctn-kix_list_9-2}.lst-kix_list_17-2>li{counter-increment:lst-ctn-kix_list_17-2}ol.lst-kix_list_17-2.start{counter-reset:lst-ctn-kix_list_17-2 0}.lst-kix_list_13-2>li{counter-increment:lst-ctn-kix_list_13-2}ol.lst-kix_list_21-5.start{counter-reset:lst-ctn-kix_list_21-5 0}.lst-kix_list_20-5>li{counter-increment:lst-ctn-kix_list_20-5}.lst-kix_list_19-7>li{counter-increment:lst-ctn-kix_list_19-7}.lst-kix_list_14-3>li{counter-increment:lst-ctn-kix_list_14-3}ol.lst-kix_list_15-6.start{counter-reset:lst-ctn-kix_list_15-6 0}.lst-kix_list_21-6>li{counter-increment:lst-ctn-kix_list_21-6}.lst-kix_list_10-3>li{counter-increment:lst-ctn-kix_list_10-3}.lst-kix_list_12-1>li{counter-increment:lst-ctn-kix_list_12-1}.lst-kix_list_18-0>li:before{content:"-  "}ol.lst-kix_list_13-3.start{counter-reset:lst-ctn-kix_list_13-3 0}.lst-kix_list_18-1>li:before{content:"-  "}.lst-kix_list_18-2>li:before{content:"-  "}ol.lst-kix_list_11-7.start{counter-reset:lst-ctn-kix_list_11-7 0}ol.lst-kix_list_14-2.start{counter-reset:lst-ctn-kix_list_14-2 0}ul.lst-kix_list_22-0{list-style-type:none}.lst-kix_list_2-7>li:before{content:"-  "}ul.lst-kix_list_22-1{list-style-type:none}ul.lst-kix_list_22-2{list-style-type:none}ul.lst-kix_list_22-3{list-style-type:none}ul.lst-kix_list_22-4{list-style-type:none}.lst-kix_list_2-5>li:before{content:"-  "}ul.lst-kix_list_22-5{list-style-type:none}ul.lst-kix_list_22-6{list-style-type:none}.lst-kix_list_17-5>li{counter-increment:lst-ctn-kix_list_17-5}ul.lst-kix_list_22-7{list-style-type:none}ol.lst-kix_list_17-0.start{counter-reset:lst-ctn-kix_list_17-0 0}ol.lst-kix_list_10-3.start{counter-reset:lst-ctn-kix_list_10-3 0}.lst-kix_list_18-6>li:before{content:"-  "}ol.lst-kix_list_9-4.start{counter-reset:lst-ctn-kix_list_9-4 0}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}.lst-kix_list_14-6>li{counter-increment:lst-ctn-kix_list_14-6}.lst-kix_list_10-1>li:before{content:"" counter(lst-ctn-kix_list_10-1,lower-latin) ". "}.lst-kix_list_21-3>li{counter-increment:lst-ctn-kix_list_21-3}.lst-kix_list_18-4>li:before{content:"-  "}.lst-kix_list_18-8>li:before{content:"-  "}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_22-8{list-style-type:none}.lst-kix_list_17-6>li{counter-increment:lst-ctn-kix_list_17-6}ul.lst-kix_list_3-2{list-style-type:none}ol.lst-kix_list_15-1.start{counter-reset:lst-ctn-kix_list_15-1 0}ol.lst-kix_list_15-4.start{counter-reset:lst-ctn-kix_list_15-4 0}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_10-7>li:before{content:"" counter(lst-ctn-kix_list_10-7,lower-latin) ". "}.lst-kix_list_20-1>li{counter-increment:lst-ctn-kix_list_20-1}.lst-kix_list_10-5>li:before{content:"" counter(lst-ctn-kix_list_10-5,lower-roman) ". "}ol.lst-kix_list_13-5.start{counter-reset:lst-ctn-kix_list_13-5 0}.lst-kix_list_10-3>li:before{content:"" counter(lst-ctn-kix_list_10-3,decimal) ". "}.lst-kix_list_15-4>li{counter-increment:lst-ctn-kix_list_15-4}ol.lst-kix_list_20-8{list-style-type:none}ol.lst-kix_list_13-8.start{counter-reset:lst-ctn-kix_list_13-8 0}ol.lst-kix_list_20-5{list-style-type:none}ol.lst-kix_list_20-4{list-style-type:none}ol.lst-kix_list_20-7{list-style-type:none}.lst-kix_list_11-7>li{counter-increment:lst-ctn-kix_list_11-7}ol.lst-kix_list_20-6{list-style-type:none}.lst-kix_list_9-2>li:before{content:"" counter(lst-ctn-kix_list_9-2,lower-roman) ". "}ol.lst-kix_list_20-1{list-style-type:none}ol.lst-kix_list_20-0{list-style-type:none}ol.lst-kix_list_20-3{list-style-type:none}ol.lst-kix_list_20-2{list-style-type:none}ol.lst-kix_list_14-0.start{counter-reset:lst-ctn-kix_list_14-0 0}.lst-kix_list_12-5>li{counter-increment:lst-ctn-kix_list_12-5}.lst-kix_list_20-8>li:before{content:"" counter(lst-ctn-kix_list_20-8,lower-roman) ". "}.lst-kix_list_9-0>li:before{content:"" counter(lst-ctn-kix_list_9-0,decimal) ". "}.lst-kix_list_20-0>li:before{content:"" counter(lst-ctn-kix_list_20-0,decimal) ". "}.lst-kix_list_9-6>li:before{content:"" counter(lst-ctn-kix_list_9-6,decimal) ". "}ol.lst-kix_list_10-7{list-style-type:none}ol.lst-kix_list_10-8{list-style-type:none}.lst-kix_list_9-4>li:before{content:"" counter(lst-ctn-kix_list_9-4,lower-latin) ". "}ol.lst-kix_list_10-3{list-style-type:none}ol.lst-kix_list_10-4{list-style-type:none}ol.lst-kix_list_10-5{list-style-type:none}.lst-kix_list_11-3>li:before{content:"" counter(lst-ctn-kix_list_11-3,decimal) ". "}ol.lst-kix_list_10-6{list-style-type:none}.lst-kix_list_20-6>li:before{content:"" counter(lst-ctn-kix_list_20-6,decimal) ". "}ol.lst-kix_list_10-0{list-style-type:none}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_10-1{list-style-type:none}ol.lst-kix_list_10-2{list-style-type:none}ol.lst-kix_list_12-1.start{counter-reset:lst-ctn-kix_list_12-1 0}.lst-kix_list_20-4>li:before{content:"" counter(lst-ctn-kix_list_20-4,lower-latin) ". "}.lst-kix_list_11-5>li:before{content:"" counter(lst-ctn-kix_list_11-5,lower-roman) ". "}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_20-2>li:before{content:"" counter(lst-ctn-kix_list_20-2,lower-roman) ". "}ol.lst-kix_list_21-2.start{counter-reset:lst-ctn-kix_list_21-2 0}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_9-8>li:before{content:"" counter(lst-ctn-kix_list_9-8,lower-roman) ". "}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_20-6>li{counter-increment:lst-ctn-kix_list_20-6}.lst-kix_list_1-1>li:before{content:"-  "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}.lst-kix_list_11-7>li:before{content:"" counter(lst-ctn-kix_list_11-7,lower-latin) ". "}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"-  "}ol.lst-kix_list_10-5.start{counter-reset:lst-ctn-kix_list_10-5 0}ol.lst-kix_list_21-8{list-style-type:none}.lst-kix_list_1-7>li:before{content:"-  "}ol.lst-kix_list_21-7{list-style-type:none}ol.lst-kix_list_21-4{list-style-type:none}.lst-kix_list_1-5>li:before{content:"-  "}ol.lst-kix_list_21-3{list-style-type:none}ol.lst-kix_list_21-6{list-style-type:none}ol.lst-kix_list_9-1.start{counter-reset:lst-ctn-kix_list_9-1 0}.lst-kix_list_14-7>li{counter-increment:lst-ctn-kix_list_14-7}ol.lst-kix_list_21-5{list-style-type:none}ol.lst-kix_list_21-0{list-style-type:none}ol.lst-kix_list_21-2{list-style-type:none}ol.lst-kix_list_21-1{list-style-type:none}.lst-kix_list_2-1>li:before{content:"-  "}.lst-kix_list_19-8>li{counter-increment:lst-ctn-kix_list_19-8}.lst-kix_list_2-3>li:before{content:"-  "}.lst-kix_list_11-8>li{counter-increment:lst-ctn-kix_list_11-8}ol.lst-kix_list_19-6{list-style-type:none}ol.lst-kix_list_19-7{list-style-type:none}ol.lst-kix_list_19-8{list-style-type:none}ol.lst-kix_list_19-2{list-style-type:none}ol.lst-kix_list_19-3{list-style-type:none}ol.lst-kix_list_19-4{list-style-type:none}ol.lst-kix_list_19-5{list-style-type:none}.lst-kix_list_20-8>li{counter-increment:lst-ctn-kix_list_20-8}ol.lst-kix_list_19-0{list-style-type:none}.lst-kix_list_9-1>li{counter-increment:lst-ctn-kix_list_9-1}ol.lst-kix_list_19-1{list-style-type:none}.lst-kix_list_3-1>li:before{content:"-  "}.lst-kix_list_3-2>li:before{content:"-  "}.lst-kix_list_14-0>li{counter-increment:lst-ctn-kix_list_14-0}.lst-kix_list_8-1>li:before{content:"-  "}ol.lst-kix_list_17-4.start{counter-reset:lst-ctn-kix_list_17-4 0}.lst-kix_list_8-2>li:before{content:"-  "}.lst-kix_list_3-5>li:before{content:"-  "}.lst-kix_list_12-0>li{counter-increment:lst-ctn-kix_list_12-0}ol.lst-kix_list_12-3.start{counter-reset:lst-ctn-kix_list_12-3 0}ol.lst-kix_list_11-5.start{counter-reset:lst-ctn-kix_list_11-5 0}.lst-kix_list_21-2>li:before{content:"" counter(lst-ctn-kix_list_21-2,lower-roman) ". "}.lst-kix_list_8-5>li:before{content:"-  "}.lst-kix_list_8-6>li:before{content:"-  "}.lst-kix_list_11-1>li{counter-increment:lst-ctn-kix_list_11-1}.lst-kix_list_15-1>li{counter-increment:lst-ctn-kix_list_15-1}.lst-kix_list_3-6>li:before{content:"-  "}.lst-kix_list_21-6>li:before{content:"" counter(lst-ctn-kix_list_21-6,decimal) ". "}.lst-kix_list_21-7>li:before{content:"" counter(lst-ctn-kix_list_21-7,lower-latin) ". "}.lst-kix_list_11-2>li:before{content:"" counter(lst-ctn-kix_list_11-2,lower-roman) ". "}.lst-kix_list_21-3>li:before{content:"" counter(lst-ctn-kix_list_21-3,decimal) ". "}ol.lst-kix_list_11-6.start{counter-reset:lst-ctn-kix_list_11-6 0}ol.lst-kix_list_12-4.start{counter-reset:lst-ctn-kix_list_12-4 0}.lst-kix_list_16-6>li:before{content:"-  "}ol.lst-kix_list_10-1.start{counter-reset:lst-ctn-kix_list_10-1 0}.lst-kix_list_17-1>li:before{content:"" counter(lst-ctn-kix_list_17-1,lower-latin) ". "}.lst-kix_list_16-1>li:before{content:"-  "}.lst-kix_list_16-2>li:before{content:"-  "}.lst-kix_list_16-5>li:before{content:"-  "}.lst-kix_list_19-3>li{counter-increment:lst-ctn-kix_list_19-3}.lst-kix_list_15-3>li{counter-increment:lst-ctn-kix_list_15-3}.lst-kix_list_12-4>li{counter-increment:lst-ctn-kix_list_12-4}ol.lst-kix_list_11-0.start{counter-reset:lst-ctn-kix_list_11-0 0}ol.lst-kix_list_10-2.start{counter-reset:lst-ctn-kix_list_10-2 0}.lst-kix_list_12-7>li{counter-increment:lst-ctn-kix_list_12-7}.lst-kix_list_17-2>li:before{content:"" counter(lst-ctn-kix_list_17-2,lower-roman) ". "}ol.lst-kix_list_17-3.start{counter-reset:lst-ctn-kix_list_17-3 0}.lst-kix_list_17-6>li:before{content:"" counter(lst-ctn-kix_list_17-6,decimal) ". "}.lst-kix_list_17-5>li:before{content:"" counter(lst-ctn-kix_list_17-5,lower-roman) ". "}.lst-kix_list_2-6>li:before{content:"-  "}.lst-kix_list_22-3>li:before{content:"\0025aa   "}.lst-kix_list_14-5>li{counter-increment:lst-ctn-kix_list_14-5}.lst-kix_list_7-1>li:before{content:"-  "}.lst-kix_list_7-5>li:before{content:"-  "}.lst-kix_list_13-5>li{counter-increment:lst-ctn-kix_list_13-5}.lst-kix_list_9-6>li{counter-increment:lst-ctn-kix_list_9-6}.lst-kix_list_19-5>li{counter-increment:lst-ctn-kix_list_19-5}.lst-kix_list_22-7>li:before{content:"\0025aa   "}ol.lst-kix_list_11-1.start{counter-reset:lst-ctn-kix_list_11-1 0}.lst-kix_list_18-5>li:before{content:"-  "}.lst-kix_list_13-6>li:before{content:"" counter(lst-ctn-kix_list_13-6,decimal) ". "}.lst-kix_list_20-3>li{counter-increment:lst-ctn-kix_list_20-3}.lst-kix_list_10-6>li{counter-increment:lst-ctn-kix_list_10-6}.lst-kix_list_11-6>li{counter-increment:lst-ctn-kix_list_11-6}ol.lst-kix_list_10-0.start{counter-reset:lst-ctn-kix_list_10-0 0}.lst-kix_list_15-6>li:before{content:"" counter(lst-ctn-kix_list_15-6,lower-latin) ". "}ol.lst-kix_list_17-8.start{counter-reset:lst-ctn-kix_list_17-8 0}.lst-kix_list_11-4>li{counter-increment:lst-ctn-kix_list_11-4}.lst-kix_list_10-2>li:before{content:"" counter(lst-ctn-kix_list_10-2,lower-roman) ". "}.lst-kix_list_13-7>li{counter-increment:lst-ctn-kix_list_13-7}.lst-kix_list_20-7>li:before{content:"" counter(lst-ctn-kix_list_20-7,lower-latin) ". "}ol.lst-kix_list_17-5.start{counter-reset:lst-ctn-kix_list_17-5 0}.lst-kix_list_4-2>li:before{content:"\0025a0   "}.lst-kix_list_4-6>li:before{content:"\0025cf   "}.lst-kix_list_17-4>li{counter-increment:lst-ctn-kix_list_17-4}.lst-kix_list_15-2>li:before{content:"" counter(lst-ctn-kix_list_15-2,decimal) ". "}.lst-kix_list_10-8>li{counter-increment:lst-ctn-kix_list_10-8}.lst-kix_list_9-1>li:before{content:"" counter(lst-ctn-kix_list_9-1,lower-latin) ". "}.lst-kix_list_10-6>li:before{content:"" counter(lst-ctn-kix_list_10-6,decimal) ". "}ol.lst-kix_list_12-7.start{counter-reset:lst-ctn-kix_list_12-7 0}.lst-kix_list_15-8>li{counter-increment:lst-ctn-kix_list_15-8}.lst-kix_list_12-2>li{counter-increment:lst-ctn-kix_list_12-2}.lst-kix_list_9-5>li:before{content:"" counter(lst-ctn-kix_list_9-5,lower-roman) ". "}.lst-kix_list_12-2>li:before{content:"" counter(lst-ctn-kix_list_12-2,decimal) ". "}ol.lst-kix_list_12-8.start{counter-reset:lst-ctn-kix_list_12-8 0}.lst-kix_list_11-6>li:before{content:"" counter(lst-ctn-kix_list_11-6,decimal) ". "}.lst-kix_list_20-3>li:before{content:"" counter(lst-ctn-kix_list_20-3,decimal) ". "}.lst-kix_list_1-2>li:before{content:"-  "}ol.lst-kix_list_11-3.start{counter-reset:lst-ctn-kix_list_11-3 0}.lst-kix_list_21-1>li{counter-increment:lst-ctn-kix_list_21-1}.lst-kix_list_10-1>li{counter-increment:lst-ctn-kix_list_10-1}ol.lst-kix_list_17-6.start{counter-reset:lst-ctn-kix_list_17-6 0}.lst-kix_list_1-6>li:before{content:"-  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_12-6>li:before{content:"" counter(lst-ctn-kix_list_12-6,lower-latin) ". "}ol.lst-kix_list_11-4.start{counter-reset:lst-ctn-kix_list_11-4 0}.lst-kix_list_2-2>li:before{content:"-  "}.lst-kix_list_13-2>li:before{content:"" counter(lst-ctn-kix_list_13-2,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c24{padding-top:18pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left;height:16pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c31{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c26{padding-top:18pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c28{padding-top:18pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c18{margin-left:36pt;padding-top:0pt;padding-bottom:8pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c29{padding-top:16pt;padding-bottom:4pt;line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c19{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center}.c44{color:#434343;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c43{padding-top:18pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;text-align:left}.c32{color:#000000;font-weight:400;text-decoration:none;font-family:"Arial";font-style:normal}.c42{padding-top:18pt;padding-bottom:12pt;line-height:1.1500000000000001;page-break-after:avoid;text-align:left}.c23{padding-top:16pt;padding-bottom:4pt;line-height:1.0;page-break-after:avoid;text-align:left}.c47{padding-top:0pt;padding-bottom:12pt;line-height:1.0;text-align:left}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c25{padding-top:0pt;padding-bottom:12pt;line-height:1.1500000000000001;text-align:left}.c41{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c0{font-size:12pt;font-family:"Cambria Math";font-weight:400}.c45{text-decoration:none;font-family:"Arial";font-style:normal}.c40{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{padding:0;margin:0}.c4{vertical-align:sub;font-size:12pt}.c11{font-family:"Quattrocento Sans";font-weight:400}.c30{orphans:2;widows:2}.c48{vertical-align:baseline;font-size:15pt}.c13{margin-left:36pt;padding-left:0pt}.c33{vertical-align:super}.c8{font-size:12pt}.c36{vertical-align:sub}.c20{margin-left:72pt}.c38{font-style:italic}.c34{font-weight:700}.c46{text-indent:36pt}.c37{color:#000000}.c49{height:14pt}.c17{margin-left:54pt}.c21{margin-left:36pt}.c9{height:11pt}.c39{font-family:"Quattrocento Sans"}.c10{font-size:9pt}.c16{padding-left:0pt}.c50{font-size:11pt}.c35{margin-left:108pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c40 doc-content"><h2 class="c28" id="h.gjdgxs"><span class="c34">MULTI-AGENT REINFORCEMENT LEARNING</span></h2><h2 class="c26"><span class="c12">Prerequisite</span></h2><p class="c5"><span class="c1">We assume that the reader knows about single agent RL settings with topics covering MDP, bandit setting, Q learning, SARSA etc. </span></p><h2 class="c26" id="h.30j0zll"><span class="c12">Introduction to MARL</span></h2><p class="c5"><span class="c1">In this blog we will talk about multi agent reinforcement learning and some of the concepts associated with it.</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c8">So first we look at what is MARL, and how is it different from single agent setting, so MARL consist of an environment with which we interact and some goals like reward maximization and also agents (</span><span class="c8 c34">Mutliple</span><span class="c1">),so the difference lies in the number of agents, One can argue that we can look at it as a single agent setting but the joint action space size will increase exponentially as the number of agent increases so we will find some algorithms to solve this problems.</span></p><p class="c5 c9"><span class="c22"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.20px; height: 300.67px;"><img alt="" src="images/image36.png" style="width: 391.20px; height: 300.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c9"><span class="c22"></span></p><p class="c5 c9"><span class="c22"></span></p><p class="c5 c9"><span class="c22"></span></p><p class="c5 c9"><span class="c22"></span></p><h2 class="c24" id="h.1fob9te"><span class="c27"></span></h2><h2 class="c24"><span class="c27"></span></h2><h2 class="c24"><span class="c27"></span></h2><h2 class="c24"><span class="c27"></span></h2><h2 class="c26"><span class="c12">Applications of MARL</span></h2><ol class="c6 lst-kix_list_10-0 start" start="1"><li class="c5 c13 li-bullet-0"><span class="c1">It has been extensively used in warehouse management where multiple robots are deployed to work together.</span></li><li class="c5 c13 li-bullet-0"><span class="c1">Multiple board games can be learned using MARL as we can use multi agents setting where each agent play for itself and exploit weakness of others, resulting in the overall improvement for all the agents</span></li><li class="c5 c13 li-bullet-0"><span class="c1">In trading also it can be used by the same principle as in board games to improve each other to trade better.</span></li></ol><p class="c5 c9"><span class="c22"></span></p><p class="c5"><span class="c1">We will move onto various classes of setting in multi agent reinforcement learning with increasing complexity in hierarchy.</span></p><h2 class="c26" id="h.3znysh7"><span class="c12">Settings in MARL</span></h2><ol class="c6 lst-kix_list_11-0 start" start="1"><li class="c29 c13 li-bullet-0"><h3 id="h.2et92p0" style="display:inline"><span class="c34 c44">Normal form games</span></h3></li></ol><p class="c5 c21"><span class="c1">Normal form games are equivalent to bandit setting in single agent RL with only 1 state.</span></p><p class="c5 c21"><span class="c1">Def : It consists of :</span></p><ol class="c6 lst-kix_list_15-0 start" start="1"><li class="c5 c20 c16 li-bullet-0"><span class="c1">Finite set of agents I</span></li><li class="c5 c20 c16 li-bullet-0"><span class="c1">For each agent i in I</span></li></ol><ul class="c6 lst-kix_list_18-0 start"><li class="c5 c16 c35 li-bullet-0"><span class="c8">Finite set of action A</span><span class="c4">i</span></li><li class="c5 c16 c35 li-bullet-0"><span class="c8">Reward function R</span><span class="c4">i </span><span class="c8">with R</span><span class="c4">i</span><span class="c8">: A -&gt; R where A = A</span><span class="c4">1</span><span class="c8">&nbsp;&times; ... &times; A</span><span class="c4">n</span></li></ul><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normal formal games can be classified on the basis of reward structure as:</span></p><ol class="c6 lst-kix_list_14-0 start" start="1"><li class="c5 c17 c16 li-bullet-0"><span class="c1">Zero-sum game : The sum of agents&#39; reward is always 0 for all joint action A. A game of rock paper scissors could be looked at as a zero sum game.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.67px; height: 29.40px;"><img alt="" src="images/image78.png" style="width: 150.67px; height: 29.40px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 c16 c17 li-bullet-0"><span class="c1">Common reward game: All the agents receive the same reward.</span></li><li class="c5 c17 c16 li-bullet-0"><span class="c1">General-sum game : No constraints</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 76.00px; height: 29.00px;"><img alt="" src="images/image28.png" style="width: 150.00px; height: 29.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><ul class="c6 lst-kix_list_16-0 start"><li class="c5 c17 c16 li-bullet-0"><span class="c1">Normal form games can be extended to repeated normal form game by repeatedly playing the game, now our policy may depend on the history denoted by </span></li></ul><p class="c5 c17"><span class="c1">rest all remains the same.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 124.67px; height: 27.70px;"><img alt="" src="images/image15.png" style="width: 124.67px; height: 27.70px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 100.67px; height: 25.17px;"><img alt="" src="images/image24.png" style="width: 100.67px; height: 25.17px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 72.33px; height: 28.38px;"><img alt="" src="images/image38.png" style="width: 72.33px; height: 28.38px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c29" id="h.tyjcwt"><span>&nbsp;2. </span><span class="c34">Stochastic Games</span></h3><p class="c5 c46"><span class="c1">Stochastic games are equivalent to MDP for the single RL setting, </span></p><p class="c5 c21"><span class="c1">Def : It consists of :</span></p><ol class="c6 lst-kix_list_12-0 start" start="1"><li class="c5 c20 c16 li-bullet-0"><span class="c1">Finite set of agents I = {1, ..., n}</span></li><li class="c5 c20 c16 li-bullet-0"><span class="c8">Finite set of states S, with subset of terminal states S&rsquo; </span><span class="c0">&sub;</span><span class="c1">&nbsp;S</span></li><li class="c5 c20 c16 li-bullet-0"><span class="c8">For each agent i </span><span class="c0">&isin;</span><span class="c1">&nbsp;I:</span></li></ol><p class="c5 c20"><span class="c1">&ndash; Finite set of actions Ai</span></p><p class="c5 c20"><span class="c1">&ndash; Reward function Ri : S &times; A &times; S &rarr; R, where A = A1 &times; ... &times; An</span></p><ol class="c6 lst-kix_list_12-0" start="4"><li class="c5 c16 c20 li-bullet-0"><span class="c1">State transition probability function T : S &times; A &times; S &rarr; [0, 1] such that</span></li></ol><p class="c5 c20"><span class="c0">&forall;</span><span class="c8">s </span><span class="c0">&isin;</span><span class="c8">&nbsp;S, a </span><span class="c0">&isin;</span><span class="c8">&nbsp;A : &sum;</span><span class="c4">s&prime;</span><span class="c0 c36">&isin;</span><span class="c4">S</span><span class="c1">T (s, a, s&prime;) = 1 </span></p><ol class="c6 lst-kix_list_12-0" start="5"><li class="c5 c20 c16 li-bullet-0"><span class="c1">Initial state distribution &mu; : S &rarr; [0, 1] such that</span></li></ol><p class="c5 c20"><span class="c8">&sum;</span><span class="c4">s</span><span class="c0 c36">&isin;</span><span class="c4">S</span><span class="c8">&mu;(s) = 1 and </span><span class="c0">&forall;</span><span class="c8">s </span><span class="c0">&isin;</span><span class="c1">&nbsp;S&rsquo; : &mu;(s) = 0</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So the game starts in some state according to &mu;, and then we select our action based on</span></p><p class="c5"><span class="c8">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy &pi;</span><span class="c4">i</span><span class="c8">(a</span><span class="c4">t</span><span class="c8">i | h</span><span class="c4">t</span><span class="c1">) for the agent i where the history contains all the states and joint actions, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c5 c21"><span class="c1">the property of history is known as full observability and then we transition according to &nbsp;our transition probability function. Also we have markov property as in MDP i.e. </span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.67px; height: 42.30px;"><img alt="" src="images/image55.png" style="width: 538.67px; height: 42.30px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c21 c9"><span class="c1"></span></p><p class="c5 c21"><span class="c1">One of the examples of stochastic games could be a foraging game where multiple agents have to collect the apples but each agent has a skill level and to collect the apple the skill level of all the agents around an apple should be more than its level. And our task is to collect all the apples as fast as possible.</span></p><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.00px; height: 297.00px;"><img alt="" src="images/image60.png" style="width: 624.00px; height: 297.00px; margin-left: -61.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c29 c21" id="h.3dy6vkm"><span>3. </span><span class="c34">Partially observable stochastic game</span></h3><p class="c5 c21"><span class="c1">So it is almost the same as a stochastic game but the property of full observability does not hold and it is more close to the natural setting as in reality many a time agents do not have full information about the state and the action of other agents.</span></p><p class="c5 c21"><span class="c1">Def: It contains all the elements of stochastic games and </span></p><p class="c5 c21"><span class="c8">&bull; Finite set of observations </span><span class="c8 c38">O</span><span class="c1">i</span></p><p class="c5 c21"><span class="c8">&bull; Observation function O</span><span class="c4">i</span><span class="c8">&nbsp;: A &times; S &times; </span><span class="c8 c38">O</span><span class="c4">i</span><span class="c1">&nbsp;&rarr; [0, 1] such that</span></p><p class="c5 c21"><span class="c0">&forall;</span><span class="c8">a </span><span class="c0">&isin;</span><span class="c8">&nbsp;A, s </span><span class="c0">&isin;</span><span class="c8">&nbsp;S : &sum;</span><span class="c4">oi</span><span class="c0 c36">&isin;</span><span class="c4 c38">O</span><span class="c4">i</span><span class="c8">O</span><span class="c4">i</span><span class="c8">(a, s, o</span><span class="c4">i</span><span class="c1">) = 1</span></p><p class="c5 c21"><span class="c8">And now the history h</span><span class="c4">t</span><span class="c8">=(o</span><span class="c4">1</span><span class="c8">,o</span><span class="c4">2</span><span class="c8">,....o</span><span class="c4">t</span><span class="c1">)</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">So the whole picture over distinct setting can be presented as </span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.15px; height: 242.67px;"><img alt="" src="images/image58.png" style="width: 417.15px; height: 242.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">also we can model communication by extending our existing action space with the help of message space</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 141.33px; height: 36.87px;"><img alt="" src="images/image65.png" style="width: 141.33px; height: 36.87px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c21"><span class="c8">where X</span><span class="c4">i</span><span class="c8">&nbsp;is the old action set and M</span><span class="c4">i</span><span class="c1">&nbsp;is the message set</span></p><p class="c25"><span class="c1">And we can also model corruption of messages or the communication error using POSG. Also remember that agents themselves don&rsquo;t understand the meaning of the messages and have to learn it through the process.</span></p><p class="c25 c9"><span class="c1"></span></p><p class="c25"><span class="c1">Now that we have seen the different types of setting in reinforcement learning, we will move to some of the equilibrium concepts where we want our policy to converge.</span></p><h2 class="c26" id="h.1t3h5sf"><span class="c12">Definition for different types of equilibrium</span></h2><p class="c5"><span class="c1">(U is the expected discounted reward)</span></p><ol class="c6 lst-kix_list_9-0 start" start="1"><li class="c5 c13 li-bullet-0"><span class="c1">Best Response:</span></li></ol><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 337.60px; height: 27.24px;"><img alt="" src="images/image61.png" style="width: 337.60px; height: 27.24px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.10px; height: 49.12px;"><img alt="" src="images/image63.png" style="width: 320.10px; height: 49.12px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c21"><span class="c1">So the best response as the name suggest is the best an agent i can do by keeping the policy of other agents fixed.</span></p><ol class="c6 lst-kix_list_9-0" start="2"><li class="c5 c13 li-bullet-0"><span class="c1">Minmax solution: In a zero-sum game with two agents, a joint policy &pi; = (&pi;i, &pi;j) is a minimax solution if</span></li></ol><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 194.00px; height: 116.40px;"><img alt="" src="images/image66.png" style="width: 194.00px; height: 116.40px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c6 lst-kix_list_9-0" start="3"><li class="c5 c13 li-bullet-0"><span class="c1">Nash Equilibrium</span></li></ol><p class="c25 c21"><span class="c1">In a general-sum game with n agents, a joint policy &pi; = (&pi;1, ..., &pi;n) is a Nash equilibrium if</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 210.67px; height: 40.38px;"><img alt="" src="images/image68.png" style="width: 210.67px; height: 50.16px; margin-left: -0.00px; margin-top: -9.77px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c21"><span class="c1">So Nash equilibrium says that we can&rsquo;t improve our expected reward unilaterally by just changing our policy.</span></p><p class="c25 c21"><span class="c1">Consider the following example of the game</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 156.60px; height: 93.28px;"><img alt="" src="images/image70.png" style="width: 156.60px; height: 93.28px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c21 c25"><span class="c1">So here the policy to perform A,A is a Nash equilibrium as if let&#39;s say agent 2 decided to change its policy to improve it will have to play B with some probability and that will reduce the overall reward. Similarly B,B is a nash equilibrium and also if both agents decided to play both A and B with half probability then also it is a nash equilibrium. So there are multiple possible nash equilibriums.</span></p><ol class="c6 lst-kix_list_9-0" start="4"><li class="c5 c13 li-bullet-0"><span class="c1">&#1108;-Nash Equilibrium</span></li></ol><p class="c5 c21"><span class="c1">Now we can loosen the constraint imposed by the nash equilibrium by using &#1108;-Nash Equilibrium which says that </span></p><p class="c25 c21"><span class="c1">In a general-sum game with n agents, a joint policy &pi; = (&pi;1, ..., &pi;n) is a &#1108;-Nash equilibrium if</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 224.67px; height: 43.53px;"><img alt="" src="images/image72.png" style="width: 224.67px; height: 43.53px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c21"><span class="c1">So we are saying that an agent can&rsquo;t improve its own expected reward by more than epsilon by changing its own policy. There is usually no relation between a nash equilibrium and &#1108;-Nash Equilibrium, consider the following example</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 205.15px; height: 102.94px;"><img alt="" src="images/image74.png" style="width: 205.15px; height: 102.94px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c21"><span class="c1">Here the action A,C is a nash equilibrium but the action B,D is an e-Nash equilibrium with e=1 as the best improvement agent 2 can do is by playing C and increase its award by epsilon.</span></p><p class="c25 c21 c9"><span class="c1"></span></p><p class="c5 c21 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_9-0" start="5"><li class="c5 c13 li-bullet-0"><span class="c1">Correlated Equilibrium</span></li></ol><p class="c5 c21"><span class="c1">Till now we have assumed probabilistically independent policy in nash equilibrium, correlated equilibrium will generalize the nash equilibrium by allowing the correlation between policy.</span></p><p class="c5 c21"><span class="c1">Definition 8 (Correlated equilibrium) In a general-sum normal-form game</span></p><p class="c5 c21"><span class="c1">with n agents, let &pi;c(a) be a joint policy that assigns probabilities to joint</span></p><p class="c5 c21"><span class="c8">actions a </span><span class="c0">&isin;</span><span class="c8">&nbsp;A. Then, &pi;c is a correlated equilibrium if for every agent i </span><span class="c0">&isin;</span><span class="c1">&nbsp;I and</span></p><p class="c5 c21"><span class="c1">every action modifier &xi;i : Ai &rarr; Ai:</span></p><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 374.67px; height: 59.78px;"><img alt="" src="images/image75.png" style="width: 374.67px; height: 59.78px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c21"><span class="c1">No agent can deviate from its recommended action to increase its expected reward.</span></p><p class="c5 c21"><span class="c1">Consider the following example</span></p><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 235.33px; height: 118.51px;"><img alt="" src="images/image76.png" style="width: 235.33px; height: 118.51px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 265.33px; height: 74.82px;"><img alt="" src="images/image77.png" style="width: 265.33px; height: 74.82px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c21"><span class="c1">here the first 3 policies are the nash equilibrium and now consider the following policy</span></p><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 238.00px; height: 59.91px;"><img alt="" src="images/image42.png" style="width: 238.00px; height: 59.91px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So if we assume that A got the recommended action as L then its expected reward is 6/2+2/2 = 4, but if it tries to change it to B then it will be reduced to 3.5 and similarly other cases followed so hence its correlated equilibrium.</span></p><p class="c5 c21 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_9-0" start="6"><li class="c5 c13 li-bullet-0"><span class="c1">Pareto Optimality</span></li></ol><p class="c5 c21"><span class="c1">Now as we have seen that there are multiple possible equillibrium so to put more constraint on the equilibrium we defines this,</span></p><p class="c25 c21"><span class="c1">Def: A joint policy &pi; is Pareto- dominated by another joint policy &pi;&prime; if</span></p><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 389.50px; height: 45.09px;"><img alt="" src="images/image44.png" style="width: 389.50px; height: 45.09px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c21"><span class="c1">So a policy is pareto efficient if it is not dominated by other policies.</span></p><p class="c5 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_9-0" start="7"><li class="c5 c13 li-bullet-0"><span class="c1">No-regret</span></li></ol><p class="c5 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 289.33px; height: 59.00px;"><img alt="" src="images/image37.png" style="width: 289.33px; height: 59.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c1">An agent is said to have no-regret if its average regret in the limit of z &rarr; &infin; is at most zero. No-regret necessitates that every agent in the game have no-regret as a solution notion.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 258.00px; height: 53.17px;"><img alt="" src="images/image12.png" style="width: 258.00px; height: 53.17px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c9"><span class="c1"></span></p><p class="c25"><span class="c1">Now we have seen various solution concepts and now we will finally move onto some of the algorithms for solving MARL.</span></p><p class="c25"><span class="c1">We will look into single agent RL reduction problems and directly apply Q learning.</span></p><ol class="c6 lst-kix_list_13-0 start" start="1"><li class="c25 c13 li-bullet-0"><span class="c19 c8">Central Q learning</span></li></ol><p class="c25 c21"><span class="c1">In this we reduce multiple agents into a single agent and then learn the policy, it suffers from large action space. And the second limitation is that in most of the MARL setting there is no central unit which guides all the agents so we have to learn an independent policy. And also here we need a notion of scalar reward r instead of individual reward which environment gives.</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 298.67px;"><img alt="" src="images/image47.png" style="width: 624.00px; height: 298.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c6 lst-kix_list_13-0" start="2"><li class="c25 c13 li-bullet-0"><span class="c19 c8">Independent Q learning</span></li></ol><p class="c25 c21"><span class="c1">In this each of the agents work on their own using Q learning algorithms and also agents don&rsquo;t use other agents&#39; action and observation in deciding their own policy. It suffers from instability as the transition probability for an agent i also depend on the other agents policy as </span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 473.33px; height: 69.03px;"><img alt="" src="images/image48.png" style="width: 473.33px; height: 69.03px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25 c21"><span class="c1">So for agent i this creates instability.</span></p><p class="c25 c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 325.33px;"><img alt="" src="images/image49.png" style="width: 624.00px; height: 325.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c1">Here is a comparison for CQl vs IQL on foraging task, we see IQL converge faster due to less size of action space</span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 609.57px; height: 352.13px;"><img alt="" src="images/image50.png" style="width: 609.57px; height: 352.13px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c22">\</span></p><h2 class="c42" id="h.4d34og8"><span class="c12">Challenges in MARL</span></h2><ol class="c6 lst-kix_list_17-0 start" start="1"><li class="c5 c13 li-bullet-0"><span class="c1">Non stationarity</span></li></ol><p class="c5 c21"><span class="c1">As we have seen in the case of IQL that MARL suffers from non-stationarity and it is true even for single agent RL setting.</span></p><ol class="c6 lst-kix_list_17-0" start="2"><li class="c5 c13 li-bullet-0"><span class="c1">Optimality of policies</span></li></ol><p class="c5 c21"><span class="c1">The optimality of policy does not guarantee the best reward we can get for example as we have seen even if we converge to a nash equilibrium it might be possibl;e that some other policy gives better reward.</span></p><ol class="c6 lst-kix_list_17-0" start="3"><li class="c5 c13 li-bullet-0"><span class="c1">Equilibrium Selection</span></li></ol><p class="c5 c21"><span class="c1">Also we have seen that from a particular kind of optimality there might be multiple equilibrium possible and it is hard to decide on which one to converge.</span></p><ol class="c6 lst-kix_list_17-0" start="4"><li class="c5 c13 li-bullet-0"><span class="c1">Credit system</span></li></ol><p class="c5 c21"><span class="c1">In the case of MARL, sometimes the success is due to the actions of multiple agents and it becomes hard to decide how to distribute rewards among them.</span></p><ol class="c6 lst-kix_list_17-0" start="5"><li class="c5 c13 li-bullet-0"><span class="c1">Scaling</span></li></ol><p class="c5 c21"><span class="c8">We have seen in the case of CQL that the actionspace increases exponentially as the number of agents increases</span><span class="c22">.</span></p><p class="c25 c9"><span class="c22"></span></p><p class="c25 c21 c9"><span class="c22"></span></p><h2 class="c26"><span class="c12">Foundational Algorithms</span></h2><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">We will look at mainly four foundational algorithms that help in dealing with the multi-agent RL problems:</span></p><p class="c5 c9"><span class="c1"></span></p><ul class="c6 lst-kix_list_8-0 start"><li class="c5 c13 li-bullet-0"><span class="c1">Joint-Action Learning</span></li><li class="c5 c13 li-bullet-0"><span class="c1">Agent Modeling</span></li><li class="c5 c13 li-bullet-0"><span class="c1">Policy-Based Learning</span></li><li class="c5 c13 li-bullet-0"><span class="c1">Regret Matching</span></li></ul><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">We will explore each of the above topics in some depth. However, before we start with this, we need to study the concept of DP which helps in forming the foundation of many algorithms that follow. Hence, let&rsquo;s start with our study of the DP approach.</span></p><p class="c5 c9"><span class="c1"></span></p><h3 class="c29" id="h.2s8eyo1"><span class="c19 c8">DP: Value Iteration</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c8">This is analogous to the classical Value Iteration algorithm that we studied for the MDPs in the case of single-agent RL. It forms the foundation for a family of temporal-difference learning algorithms. It is worth mentioning that this approach requires that the reward function R</span><span class="c4">i</span><span class="c1">&nbsp;for each agent, and the state transition function T for the game should be known. The following pseudocode helps in understanding the implementation of this approach:</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c8">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 296.00px;"><img alt="" src="images/image51.png" style="width: 624.00px; height: 341.43px; margin-left: -0.00px; margin-top: -45.43px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c8">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c7">Fig 1: Pseudocode for DP approach in Value iteration</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c8">We start with initializing the Value function for each state with zero (or randomly). We repeat two update processes for V</span><span class="c4">i</span><span class="c8">(s) and M</span><span class="c4">i,s</span><span class="c8">(a) respectively. Here, M</span><span class="c4">i,s</span><span class="c8">(a) represents the reward function for agent i in the normal-form game for state s (if we fix the state s). In the second update equation, Value(M</span><span class="c4">1,s</span><span class="c8">, &hellip; , M</span><span class="c4">n,s</span><span class="c1">) represents the minimax value for agent i. We perform these update steps until these values converge to their optimal values.</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">Also, note that in case of a single-agent, the second update equation reduces to the normal Value Iteration updation step:</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 149.33px;"><img alt="" src="images/image52.png" style="width: 624.00px; height: 149.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c7">Figure 2: DP update equation in case of single-agent game</span></p><h3 class="c29" id="h.17dp8vu"><span class="c19 c8">DP: Proof of Convergence</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">We mentioned in the previous paragraph that the values in the pseudocode of Figure 1 will converge to their optimal values. However, this assumption is not trivial. Nevertheless, we can prove this assumption using a similar approach that we used to prove the convergence of Value Iteration in the class. We define the following two concepts: </span></p><p class="c5 c9"><span class="c1"></span></p><ul class="c6 lst-kix_list_5-0 start"><li class="c15 c13 li-bullet-0"><span class="c8">Contraction Mapping: A mapping f: S-&gt;S defined for a || . ||-normed complete vector space S is a </span><span class="c0">&#8509;</span><span class="c8">-contraction, for </span><span class="c0">&#8509;</span><span class="c1">&nbsp;&#1013; [0,1), if for all x,y &#1013; S:</span></li></ul><p class="c15 c9"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 339.40px; height: 56.57px;"><img alt="" src="images/image25.png" style="width: 339.40px; height: 56.57px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">FIgure 3: Contraction Mapping</span></p><p class="c3 c9"><span class="c1"></span></p><ul class="c6 lst-kix_list_4-0 start"><li class="c15 c13 li-bullet-0"><span class="c1">Banach fixed-point theorem, if f is a contraction mapping, then for any initial vector x &#1013; S, the sequence f(x),f(f(x)),f(f(f(x))),... converges to a unique fixed point x* &#1013; S such that f(x*)=x*.</span></li></ul><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">Using the max-norm ||x||</span><span class="c4">&infin;</span><span class="c8">&nbsp;= max</span><span class="c4">i</span><span class="c8">&nbsp;|x</span><span class="c4">i</span><span class="c8">|, it can be shown that the value update operator in our Value Iteration algorithm satisfies the characteristics of a contraction mapping. Thus, repeated application of the value update operator in the pseudocode of Figure 1, leads to convergence to a unique fixed point which, by definition, is given by V*</span><span class="c4">i</span><span class="c1">&nbsp;for all agents i &#1013; I.</span></p><hr style="page-break-before:always;display:none;"><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 150.67px;"><img alt="" src="images/image26.png" style="width: 624.00px; height: 150.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c10">Figure 4: Optimal values V*</span><span class="c36 c10">i</span><span class="c10">(s) and M*</span><span class="c36 c10">i,s</span><span class="c7">(a)</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">Thus, we have a guaranteed convergence at the end. However, as mentioned earlier, this approach requires access to the game model, i.e., the reward model R</span><span class="c4">i</span><span class="c1">&nbsp;for each agent i, and the state transition function for the game which not be always available. This is a major drawback of this basic approach. Nevertheless, this approach forms the basis of different Temporal differencing techniques that we shall see in the next section.</span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c23" id="h.3rdcrjn"><span class="c19 c8">&nbsp;Temporal Differencing: Joint-Action Learning</span></h3><h3 class="c23 c49" id="h.26in1rg"><span class="c1"></span></h3><p class="c15"><span class="c8">A family of MARL algorithms based on the technique of Temporal Differencing. Similar to the Q(s,a) values learned in the case of single-agent MDP environments, this technique learns the joint-action value functions that estimate the expected returns of joint actions in any given state. Analogous to the Bellman equation for MDPs, these algorithms help in learning the expected return for an agent i in state s, when it selects a joint action a</span><span class="c4">i</span><span class="c1">&nbsp;while following a policy &pi;.</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">As we will soon see, JAL algorithms are off-policy algorithms which aim to learn equilibrium Q-values, Q</span><span class="c8 c33">&pi;</span><span class="c4">i</span><span class="c1">* , where &pi;* is an equilibrium joint policy for the stochastic game. The update equation for the Q-values is given as follows:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 92.00px;"><img alt="" src="images/image27.png" style="width: 624.00px; height: 92.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 5: Update equation for the state-action function for agent i, following a policy &pi;</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">Note that this case is significantly different from the single-agent MDP cases since we cannot simply take argmax</span><span class="c4">ai</span><span class="c8">Q</span><span class="c4">i</span><span class="c8">(s, a</span><span class="c4">1</span><span class="c8">, a</span><span class="c4">2</span><span class="c8">,....,a</span><span class="c4">n</span><span class="c8">) for action-selection. The action-selection process requires additional assumptions about the other agents&rsquo; actions to select optimal actions and compute TD target values. Also, as we shall see shortly, different equilibria might yield different expected returns. </span><hr style="page-break-before:always;display:none;"></p><p class="c15"><span class="c8">If we fix the state s, the set of joint-action values Q</span><span class="c4">1</span><span class="c8">(s, . ),...,Q</span><span class="c4">n</span><span class="c8">(s, . ) can be viewed as a non-repeated normal-form game </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c1">&nbsp;for state s, in which the reward function for agent i is given by:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 427.00px; height: 56.00px;"><img alt="" src="images/image29.png" style="width: 427.00px; height: 56.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 6: Reward function for agent i in the non-repeated normal-form game obtained by fixing the state s</span></p><p class="c3 c9"><span class="c1"></span></p><p class="c15"><span class="c8">For example, given a stochastic game with two agents (say i and j), and three possible actions for each agent, the normal form </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c8">&nbsp;= {</span><span class="c11 c8">&#11396;</span><span class="c4">s,i,</span><span class="c8">, </span><span class="c11 c8">&#11396;</span><span class="c4">s,j</span><span class="c1">} in state s can be written as:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 189.33px;"><img alt="" src="images/image30.png" style="width: 624.00px; height: 189.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 7: Description of normal-form game component for agent i</span></p><p class="c3 c9"><span class="c7"></span></p><p class="c15"><span class="c8">where a</span><span class="c4">i,k</span><span class="c8">&nbsp;denotes that the agent i has performed the action k. Similarly, we can define </span><span class="c11 c8">&#11396;</span><span class="c4">s,j</span><span class="c8">&nbsp;for the agent j. Hence, the combination of </span><span class="c11 c8">&#11396;</span><span class="c4">s,i</span><span class="c8">&nbsp;and </span><span class="c11 c8">&#11396;</span><span class="c4">s,j</span><span class="c8">&nbsp;together represents the non-repeated normal-form game </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c1">. The pseudocode for the JAL-GT algorithm is given as follows:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 281.33px;"><img alt="" src="images/image31.png" style="width: 624.00px; height: 281.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 8: Pseudocode for JAL-GT algorithm</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">As shown in Fig 8, we first initialize the Q(s,a) values for each agent i, and each state-action pair (s,a). For each time step, we first observe the current state, and then we follow an infinite exploration strategy (such as &#1013;-greedy) to sample actions. With probability &#1013;, we select an action randomly, otherwise, we sample action by the policy obtained from solving the normal-form game </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c1">&nbsp;corresponding to the current state s. Next, we observe the joint actions for each agent, and get the joint rewards along with the next state. Finally, we update the Q(s,a) values for each agent.</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">Solving for </span><span class="c11 c8">&#11396;</span><span class="c4">s </span><span class="c8">gives the equilibrium joint policy &pi;*</span><span class="c4">s</span><span class="c8">&nbsp;for a non-repeated normal form game, while &pi;* is the equilibrium joint policy we want to learn for the stochastic game. Hence, we earlier called the JAL-GT algorithm as an off-policy learning method since the joint policy &pi;*</span><span class="c4">s </span><span class="c8">obtained by solving </span><span class="c11 c8">&#11396;</span><span class="c4">s </span><span class="c8">helps in obtaining the equilibrium joint policy &pi;* that we want to learn. Given &pi;*</span><span class="c4">s</span><span class="c8">, action-selection can now be done in state s by sampling the joint action a ~ &pi;*</span><span class="c4">s </span><span class="c1">with some random exploration (as shown in the pseudocode of Figure 7)</span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c23" id="h.lnxbz9"><span class="c8 c34 c37">Methods to Solve </span><span class="c8 c34 c37 c39">&#11396;</span><span class="c4 c34 c37 c45">s</span></h3><p class="c15"><span class="c4">There are three main methods to solve the non-repeated normal-form game </span><span class="c8 c11">&#11396;</span><span class="c4 c32">s that we obtained in our pseudocode in Figure 7. These are:</span></p><ul class="c6 lst-kix_list_3-0 start"><li class="c15 c13 li-bullet-0"><span class="c4">Minimax Q-Learning</span></li><li class="c15 c13 li-bullet-0"><span class="c4">Nash Q-Learning</span></li><li class="c15 c13 li-bullet-0"><span class="c4">Correlated Q-Learning</span></li></ul><p class="c15"><span class="c32 c4">&nbsp;We will have a look at each of the three algorithms one-by-one.</span></p><p class="c5 c9"><span class="c1"></span></p><h3 class="c29" id="h.35nkun2"><span class="c8 c19">JAL-GT: Minimax Q-Learning</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c8">This method solves for </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c33 c8">&nbsp;</span><span class="c8">by computing a minimax solution. It is guaranteed to learn the unique minimax value of the stochastic game. We referred to the paper </span><span class="c8 c38">Markov games as a framework for multi-agent reinforcement learning </span><span class="c1">(Littman, 1994) where the researchers created a simplified grid-world environment with two agents (as shown in Fig 9). </span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.20px; height: 286.77px;"><img alt="" src="images/image32.png" style="width: 507.20px; height: 286.77px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 9: Simplified grid-world soccer game with two agents A and B</span></p><p class="c3"><span class="c7">Source: Markov games as a framework for multi-agent reinforcement learning (Littman, 1994)</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">The grid-world consists of two agents A and B and represents a zero-sum stochastic game. After training the model using two algorithms: minimax Q-Learning and independent Q-Learning, the researchers tested the learned policies against three models:</span></p><p class="c15 c9"><span class="c1"></span></p><ul class="c6 lst-kix_list_6-0 start"><li class="c15 c13 li-bullet-0"><span class="c1">A random model that chooses actions randomly uniformly</span></li><li class="c15 c13 li-bullet-0"><span class="c1">A hand-built deterministic algorithm based upon heuristics</span></li><li class="c15 c13 li-bullet-0"><span class="c1">An optimal opponent that uses Q-Learning to select optimal actions against fixed-policy agents</span></li></ul><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">The results obtained are mentioned in the following figure:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 590.06px; height: 215.71px;"><img alt="" src="images/image33.png" style="width: 590.06px; height: 215.71px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 10: Results obtained from the grid-world soccer environment gameplay</span></p><p class="c3 c9"><span class="c1"></span></p><p class="c15"><span class="c1">We can see that the models perform decently against the first two opponents. However, in the case of the optimal (worst-case) opponent, they fail miserably. As we see shortly, this can be due to the fixed-policy execution by the minimax Q-Learning models against an optimal model which optimizes its actions based upon the actual actions of its opponent. </span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c29" id="h.1ksv4uv"><span class="c19 c8">JAL-GT: Nash Q-Learning</span></h3><p class="c5 c9"><span class="c22"></span></p><p class="c15"><span class="c1">This method is guaranteed to learn a Nash equilibrium of the stochastic game under two highly restrictive assumptions that all the encountered normal-games should have either a global optimum (an equilibrium where all agents have maximum expected return) or a saddle point (an equilibrium where if any agent deviates from the joint policy, all other agents receive a higher expected return). These are highly restrictive and impractical assumptions because these two are significantly stronger assumptions and might not always exist in most of the normal-form games.</span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c29" id="h.44sinio"><span class="c19 c8">JAL-GT: Correlated Q-Learning</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c8">This method solves the normal-form game </span><span class="c11 c8">&#11396;</span><span class="c4">s</span><span class="c1">&nbsp;by computing a correlated equilibrium, and can be applied to general-sum stochastic games with a finite number of agents. It presents two main benefits over Nash Q-Learning:</span></p><ul class="c6 lst-kix_list_7-0 start"><li class="c15 c13 li-bullet-0"><span class="c1">A wider solution space</span></li><li class="c15 c13 li-bullet-0"><span class="c1">More efficient computation via Linear Programming (as compared to Quadratic Programming in the case of Nash Q-Learning)</span></li></ul><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">A wider solution space, however, means that the equilibrium selection problem will now be more pronounced. We looked at the original paper on </span><span class="c8 c38">Correlated Q-Learning </span><span class="c1">(2003) where the authors propose several ways to solve this problem. One such way is to choose an equilibrium that maximizes the sum of expected rewards of the agents.</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">Another possible option is to select an equilibrium that maximizes the min/max of the agents&rsquo; expected rewards.</span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c23" id="h.2jxsxqh"><span class="c19 c8">Joint-Action Learning: Limitations</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c1">There exist &ldquo;turn-taking&rdquo; stochastic games (games in which at each time-step, only one agent has the option to select among multiple actions, and other agents can select only one action) that have a unique stationary probabilistic equilibrium, but no stationary deterministic equilibrium. In a &ldquo;turn-taking&rdquo; stochastic game, any JAL-GT algorithm would attempt to learn a stationary deterministic equilibrium (joint policy). In the paper Cyclic Equilbria in Markov Games (Zinkevich, Greenwald, and Littman, 2005), the authors show the examples of what they call as the NoSDE (No Stationary Deterministic Equilibrium) stochastic games, in which there doesn&rsquo;t exist a stationary deterministic equilibrium for the stochastic game. One such example is shown in the following figure: </span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 574.71px; height: 250.75px;"><img alt="" src="images/image34.png" style="width: 574.71px; height: 250.75px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 11: Example of an NoSDE (No Stationary Deterministic Equilibrium)</span></p><p class="c3"><span class="c7">Source: Cyclic Equilibria in Markov Games (Zinkevich, Greenwald, and Littman, 2005)</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">In figure 11, we have a simple stochastic game with two agents. Any possible deterministic policy (basically, a combination of send/keep actions for the two agents) will not lead to an equilibrium because one of the agents can always deviate from the policy to get higher expected returns. However, the authors show that there exists a stationary probabilistic equilibrium for this game that cannot be obtained using Joint-Action Learning algorithms. Hence, now we look at the Agent Modeling algorithms which approach the problem of solving stochastic games by modeling the actions of other agents and then selecting the best possible action. </span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c23" id="h.z337ya"><span class="c19 c8">Agent Modeling</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c1">These algorithms are concerned with constructing models of other agents that can make useful predictions about their behaviors. This method is known as Policy Reconstruction, which aims to learn models of policies of other agents based on their observed past actions. Given a set of models of other agents&rsquo; policies, a given agent can select a best-response policy with respect to these models. We will look at three variants of agent modeling algorithms which are mentioned as follows:</span></p><ul class="c6 lst-kix_list_1-0 start"><li class="c5 c13 li-bullet-0"><span class="c1">Fictitious Play</span></li><li class="c5 c13 li-bullet-0"><span class="c1">Joint Action Learning through Agent Modeling </span></li><li class="c5 c13 li-bullet-0"><span class="c1">Bayesian Learning and Value of Information</span></li></ul><p class="c5 c9"><span class="c1"></span></p><h3 class="c29" id="h.3j2qqm3"><span class="c19 c8">Agent Modeling: Fictitious Play</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c1">Each agent i models the policy of every other agent j as a stationary probability distribution by taking the empirical distribution of agent j&rsquo;s past actions: </span></p><p class="c9 c15"><span class="c1"></span></p><p class="c15 c9"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 292.00px; height: 107.00px;"><img alt="" src="images/image35.png" style="width: 292.00px; height: 107.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c10">Fig 12: Update equation for Probability distribution of agent j taking the action a</span><span class="c36 c10">j</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c8">As shown in figure 12, the probability distribution of the agent j taking action a</span><span class="c4">j</span><span class="c8">&nbsp;depends on C(a</span><span class="c4">j</span><span class="c8">): number of times the agent j takes action a</span><span class="c4">j</span><span class="c1">&nbsp;before the current episode. This method is defined for non-repeated normal-form games only as it doesn&rsquo;t take into consideration the states in which given action was taken by an agent. Still it has interesting convergence properties so that the empirical distributions converge towards a final equilibrium even in randomized equilibrium cases. In each episode, agent i selects the best response depending upon the other agents&rsquo; model estimates</span></p><p class="c5 c9"><span class="c1"></span></p><h3 class="c29" id="h.1y810tw"><span class="c19 c8">Agent Modeling: Joint-Action Learning </span></h3><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">These algorithms Extend the idea of agent modeling to general stochastic games using Joint-Action Learning. These methods learn other agents&rsquo; empirical distributions based upon their past actions and the state in which those actions took place:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 322.00px; height: 98.00px;"><img alt="" src="images/image13.png" style="width: 322.00px; height: 98.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c10">Fig 13: Update equation for probability distribution of agent j taking action a</span><span class="c10 c36">j</span><span class="c7">&nbsp;in state s</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">As shown in Fig 13, the probability distribution update equation for agent j takes into consideration both the action and the state in which this action was taken. Here, C(s, a</span><span class="c4">j</span><span class="c8">) represents the number of times the agent j took action a</span><span class="c4">j</span><span class="c1">&nbsp;when it was present in state s. We have the following pseudocode for the JAL-AM algorithm:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 576.16px; height: 360.19px;"><img alt="" src="images/image14.png" style="width: 576.16px; height: 394.15px; margin-left: -0.00px; margin-top: -33.96px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 14: Pseudocode for JAL-AM algorithm</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">As shown in Fig 14, we first initialize the Q(s,a) values and the probability distributions randomly. Then, we repeat for each time step a number of steps which include first observing the current state, and then action-selection. The latter is performed using an infinite exploration strategy (such as &#1013;-greedy) where with probability &#1013;, the action is selected randomly, otherwise, the best-response action is selected based upon the other agents&rsquo; probability distributions. After observing the joint action, updates are made to the probability distribution and Q(s,a) values for the agents. </span></p><hr style="page-break-before:always;display:none;"><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">The following figure presents a comparison of evaluation returns of three different algorithms (Independent Q-Learning, Correlated Q-Learning, and JAL-AM) on the famous foraging environment: </span></p><p class="c15 c9"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.81px; height: 319.57px;"><img alt="" src="images/image16.png" style="width: 519.81px; height: 319.57px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 15: Evaluation returns of IQL, CQL and JAL-AM on the foraging environment</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">As we can see in fig 15, The agent models learned by the JAL-AM agents reduced the variance in their update targets, as is reflected by the smaller standard deviation in evaluation returns compared to IQL and CQL. </span></p><p class="c15 c9"><span class="c1"></span></p><h3 class="c23" id="h.4i7ojhp"><span class="c19 c8">Agent Modeling: Bayesian Learning and Value of Information</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c15"><span class="c8">These methods maintain the empirical distributions of other agents&rsquo; models along with their associated likelihoods. We define a metric called Value of Information (VI) that evaluates how the outcomes of an action may influence the learning agent&rsquo;s beliefs about the other agents, and how the changed beliefs will in turn influence the future actions of the learning agent. Based upon the probability distribution of other agents and their corresponding likelihoods, these methods compute best-response actions such that it maximizes the Value of Information. After observing agent j&rsquo;s action a</span><span class="c33 c8">t</span><span class="c4">j</span><span class="c8">&nbsp;in state s</span><span class="c33 c8">t</span><span class="c1">, agent i updates its belief by computing a Bayesian posterior distribution:</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 108.00px;"><img alt="" src="images/image17.png" style="width: 624.00px; height: 108.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c7">Fig 16: Update equation for the likelihood of agent j&rsquo;s probability distribution model, given the history of states and </span></p><p class="c3 c9"><span class="c7"></span></p><p class="c5"><span class="c19 c50">Limitations of JAL algorithms</span></p><ol class="c6 lst-kix_list_19-0 start" start="1"><li class="c2 c16 li-bullet-0"><span class="c1">Joint action values learnt from JAL-GT algorithms might not contain sufficient information to identify join optimal policy</span></li><li class="c18 c16 li-bullet-0"><span class="c1">Fictitious play and JAL-AM are unable to represent probabilistic policies since they use deterministic best response functions</span></li></ol><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c19 c8">Policy Based Learning</span></p><p class="c5"><span class="c1">Policy based learning helps us to directly learn the optimal policy of the agents. We can also represent probabilistic policies using this method.</span></p><p class="c5"><span class="c1">Let us see some of the methods which use Policy based learning:</span></p><ol class="c6 lst-kix_list_20-0 start" start="1"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Gradient ascent in expected reward</span></li></ol><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">Let there be two agents i and j, and let their reward matrices be as follows:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 339.83px; height: 64.45px;"><img alt="" src="images/image18.png" style="width: 339.83px; height: 64.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">We assume that both agents have only two possible actions, so let us represent the policies of the agents as follows:</span></p><p class="c2"><span class="c1">&nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 248.97px; height: 22.49px;"><img alt="" src="images/image19.png" style="width: 248.97px; height: 22.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Given this, we can write expected reward for each of the agents as follows:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 318.50px; height: 47.46px;"><img alt="" src="images/image20.png" style="width: 318.50px; height: 47.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 327.59px; height: 44.65px;"><img alt="" src="images/image21.png" style="width: 327.59px; height: 44.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Where</span></p><p class="c2"><span class="c1">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 153.49px; height: 52.09px;"><img alt="" src="images/image22.png" style="width: 153.49px; height: 52.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">We can do gradient ascent as follows:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 184.30px; height: 77.70px;"><img alt="" src="images/image23.png" style="width: 184.30px; height: 77.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Where </span><img src="images/image1.png"><span class="c1">&nbsp;is the step size.</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">One thing to note here is that if it ever happens than </span><img src="images/image2.png"><span class="c1">&nbsp;end up outside the range of [0,1], we project them back to the range.</span></p><p class="c2 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_20-0" start="2"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Infinitesimal Gradient Ascent Learning Dynamics</span></li></ol><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">If we consider the update as continuous, then we get the following update equation:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 304.05px; height: 72.11px;"><img alt="" src="images/image54.png" style="width: 304.05px; height: 72.11px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">It is clear that the convergence point would be the point of zero gradient. Setting LHS as 0, we get:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 193.67px; height: 45.92px;"><img alt="" src="images/image56.png" style="width: 193.67px; height: 45.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">However, convergence is not achieved in all cases. But it depends on the nature of the matrix F as follows:</span></p><p class="c2"><span class="c1">Case 1: F is not invertible- In this case, we get a divergent trajectory for &nbsp;</span><img src="images/image3.png"><span class="c1">. </span></p><p class="c2"><span class="c1">Case 2: F has both real eigenvalues- In this case, we get a divergent trajectory to and away from the center point.</span></p><p class="c2"><span class="c1">Case 3: F has imaginary eigenvalues- We get an elliptical trajectory around the center point. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 444.91px; height: 193.13px;"><img alt="" src="images/image57.png" style="width: 444.91px; height: 193.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">Also, when </span><img src="images/image3.png"><span class="c1">&nbsp;converge, we get a Nash equilibrium policy, but when there is no convergence, then average reward during learning converge to expected reward of some Nash equilibrium.</span></p><p class="c2 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_20-0" start="3"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Win or Learn fast</span></li></ol><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">It turns out that in the previous method, it is the constant learning rate which prevents convergence. If we allow learning rate to vary, then it is possible to have a set of learning rates such that we always converge to the optimal policy.</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">We use the following update equations:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 236.74px; height: 103.85px;"><img alt="" src="images/image59.png" style="width: 236.74px; height: 103.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Where </span><img src="images/image4.png"><span class="c1">, and when agent is winning, we use </span><img src="images/image5.png"><span class="c1">, when agent is losing, we use </span><img src="images/image6.png"><span class="c1">. Hence the name, win or learn fast. The idea is that when the agent is losing, it should do some changes quickly in order to win, and when the agent is winning, it should change its policy slowly, since the losing agent is expected to change its policy quickly.</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">Below is the graph of convergence when we add this modification for the case of all imaginary eigenvalues:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 241.20px; height: 185.62px;"><img alt="" src="images/image62.png" style="width: 241.20px; height: 185.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c6 lst-kix_list_20-0" start="4"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Win or Learn Fast with Policy Hill Climbing</span></li></ol><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">The above discussed methods require complete knowledge of agent&rsquo;s reward function and policy of the other agent, which is too much to ask for in a typical RL setting. Win or Learn Fast with Policy Hill Climbing </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 431.05px; height: 461.28px;"><img alt="" src="images/image64.png" style="width: 431.05px; height: 461.28px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c18"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 341.69px; height: 145.41px;"><img alt="" src="images/image67.png" style="width: 341.69px; height: 145.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is the convergence graph of WoLF-PHC.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 330.22px; height: 290.40px;"><img alt="" src="images/image69.png" style="width: 330.22px; height: 290.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c18 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c19 c8">No Regret Learning</span></p><p class="c5"><span class="c1">This method also computes action probabilities but using the definitions of unconditional and conditional regret.</span></p><p class="c5"><span class="c1">Let us look at them separately:</span></p><ol class="c6 lst-kix_list_21-0 start" start="1"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Unconditional regret matching</span></li></ol><p class="c2"><span class="c1">It simply computes action probabilities proportional to the unconditional regret of the action. So, more is the regret corresponding to an action, more is the probability of selecting that action. </span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">This equation gives the unconditional regret for action a_i after z episodes.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 219.50px; height: 56.15px;"><img alt="" src="images/image71.png" style="width: 219.50px; height: 56.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">We then average out the regret as follows:</span></p><p class="c2"><span class="c1">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 124.60px; height: 47.36px;"><img alt="" src="images/image73.png" style="width: 124.60px; height: 47.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Action probabilities are computed as follows:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 165.52px; height: 41.60px;"><img alt="" src="images/image53.png" style="width: 165.52px; height: 41.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c9"><span class="c1"></span></p><ol class="c6 lst-kix_list_21-0" start="2"><li class="c2 c16 li-bullet-0"><span class="c19 c8">Conditional regret matching</span></li></ol><p class="c2"><span class="c1">Here we compute action probabilities proportional to the conditional regret of the most recently selected action. The regret and policy calculations are as follows:</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">Below is the regret for not having chosen action </span><img src="images/image7.png"><span class="c1">&nbsp;and instead choosing </span><img src="images/image8.png"><span class="c1">:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.84px; height: 50.14px;"><img alt="" src="images/image39.png" style="width: 286.84px; height: 50.14px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">We then average out the regret similar to unconditional regret matching:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 165.60px; height: 48.65px;"><img alt="" src="images/image40.png" style="width: 165.60px; height: 48.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.90px; height: 58.76px;"><img alt="" src="images/image41.png" style="width: 298.90px; height: 58.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Where </span><img src="images/image9.png"><span class="c1">&nbsp;is the action chosen in the last episode z, </span><img src="images/image10.png"><span class="c1">&nbsp;is a parameter.</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">It can be shown that both conditional and unconditional regret matching&rsquo;s average regrets are bounded by </span><img src="images/image11.png"><span class="c1">&nbsp;for some constant </span><img src="images/image1.png"><span class="c1">.</span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">Below is the graph of convergence of no regret learning for a rock-paper scissor game:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 481.71px; height: 208.99px;"><img alt="" src="images/image43.png" style="width: 481.71px; height: 208.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c9"><span class="c1"></span></p><p class="c2"><span class="c1">In regret matching, only the average policy converges to the optimal policy, and convergence is weaker than in the case of WoLF-PHC. Also, we see oscillatory behaviour near the convergence points as follows:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 490.31px; height: 163.13px;"><img alt="" src="images/image45.png" style="width: 490.31px; height: 163.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c9"><span class="c1"></span></p><p class="c18 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p><h2 class="c43" id="h.2xcytpi"><span class="c32 c48">Future Work</span></h2><p class="c5 c9"><span class="c1"></span></p><h3 class="c29" id="h.1ci93xb"><span class="c19 c8">Grid-World Soccer Environment</span></h3><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">Our first aim will be to revisit the grid-world soccer environment that we saw earlier in the section on Joint-Action Learning via Minimax Q-Learning. We will first describe the environment completely, look at the original results and then explain our future plan with this environment. </span></p><p class="c5 c9"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 361.77px; height: 204.54px;"><img alt="" src="images/image32.png" style="width: 361.77px; height: 204.54px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span class="c7">Fig 17: Simplified grid-world soccer game with two agents A and B</span></p><p class="c3"><span class="c7">Source: Markov games as a framework for multi-agent reinforcement learning (Littman, 1994)</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c8">The environment shown in Fig 17 represents a zero-sum stochastic game with two agents. It is played on a 4x5 soccer grid, starting in an initial state. The ball is randomly assigned to an agent and there are 5 possible actions for each agent at every time step: up, down, right, left, or stand still. Further, an agent loses the ball if it moves to a location occupied by another agent. An agent scores +1 reward if it moves into the opponent goal (-1 reward for the opponent), and this marks the end of an episode. Each episode terminates as a draw with probability 0.1 in each time step (</span><span class="c0">&#8509;</span><span class="c1">=0.9). After training, one of the learned policies from each algorithm was tested over 100,000 time steps against three different models:</span></p><ul class="c6 lst-kix_list_6-0"><li class="c15 c13 li-bullet-0"><span class="c1">A random model that chooses actions randomly uniformly</span></li><li class="c15 c13 li-bullet-0"><span class="c1">A hand-built deterministic algorithm based upon heuristics</span></li><li class="c15 c13 li-bullet-0"><span class="c1">An optimal opponent that uses Q-Learning to select optimal actions against fixed-policy agents</span></li></ul><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span class="c1">We looked over the results briefly in the section on Minimax Q-Learning earlier, and now, we plan the following tasks as we move forward:</span></p><p class="c15 c9"><span class="c1"></span></p><ul class="c6 lst-kix_list_2-0 start"><li class="c41 c13 li-bullet-0"><span class="c1">Replicate the original paper and implement the Minimax Q-Learning algorithm for this soccer environment</span></li><li class="c13 c41 li-bullet-0"><span class="c1">Implement Deep Q-Networks for this soccer environment</span></li><li class="c25 c13 li-bullet-0"><span class="c1">Compare the results of the policies learned by both the algorithms</span></li></ul><p class="c25 c9"><span class="c1"></span></p><p class="c25"><span class="c19 c8">Rock Paper Scissor Environment</span></p><ul class="c6 lst-kix_list_22-0 start"><li class="c15 c30 c13 li-bullet-0"><span class="c1">Implement JAL-AM and WoLF-PHC</span></li><li class="c15 c13 c30 li-bullet-0"><span class="c1">Compare JAL-AM, WoLF-PHC and No regret learning in Rock, Paper and Scissor Game</span></li><li class="c15 c30 c13 li-bullet-0"><span class="c1">Compare their convergence properties like time taken, smoothness, etc</span></li><li class="c30 c13 c47 li-bullet-0"><span class="c1">Compare the policies which they converge to in terms of reward</span></li></ul><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 230.00px; height: 220.27px;"><img alt="Rock paper scissors - Wikipedia" src="images/image46.png" style="width: 230.00px; height: 220.27px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></body></html>